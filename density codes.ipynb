{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "!unzip tiny-imagenet-200.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 20\n",
    "#NUM_CLASSES = 200\n",
    "#NUM_IMAGES_PER_CLASS = 500\n",
    "NUM_IMAGES = 100000\n",
    "TRAINING_IMAGES_DIR = './tiny-imagenet-200/train/'\n",
    "#TRAIN_SIZE = NUM_IMAGES\n",
    "\n",
    "#NUM_VAL_IMAGES = 9832\n",
    "#VAL_IMAGES_DIR = './tiny-imagenet-200/val/'\n",
    "IMAGE_SIZE = 64\n",
    "NUM_CHANNELS = 3\n",
    "IMAGE_ARR_SIZE = IMAGE_SIZE * IMAGE_SIZE * NUM_CHANNELS\n",
    "def load_training_images(image_dir, batch_size=500):\n",
    "\n",
    "    image_index = 0\n",
    "    images = np.ndarray(shape=(NUM_IMAGES, IMAGE_ARR_SIZE))\n",
    "    names = []\n",
    "    labels = []                       \n",
    "    \n",
    "    print(\"Loading training images from \", image_dir)\n",
    "    # Loop through all the types directories\n",
    "    for type in os.listdir(image_dir):\n",
    "        if os.path.isdir(image_dir + type + '/images/'):\n",
    "\n",
    "            type_images = os.listdir(image_dir + type + '/images/')\n",
    "            # Loop through all the images of a type directory\n",
    "            batch_index = 0;\n",
    "            #print (\"Loading Class \", type)\n",
    "            for image in type_images:\n",
    "                image_file = os.path.join(image_dir, type + '/images/', image)\n",
    "\n",
    "                # reading the images as they are; no normalization, no color editing\n",
    "                image_data = cv2.imread(image_file) \n",
    "                image_data = np.array(image_data)\n",
    "                #print ('Loaded Image', image_file, image_data.shape)\n",
    "                if (image_data.shape == (IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)):\n",
    "                    images[image_index, :] = image_data.flatten()\n",
    "\n",
    "                    labels.append(type)\n",
    "                    names.append(image)\n",
    "                    \n",
    "                    image_index += 1\n",
    "                    batch_index += 1\n",
    "                if (batch_index >= batch_size):\n",
    "                    break;\n",
    "    \n",
    "    print(\"Loaded Training Images\", image_index)\n",
    "    return (images, np.asarray(labels), np.asarray(names))\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "data = load_training_images(TRAINING_IMAGES_DIR, batch_size=500)\n",
    "seed = np.random.randint(0,100000,10000)\n",
    "data = data[0][seed]\n",
    "TIM = data\n",
    "data = 1\n",
    "TIM = np.reshape(TIM,[10000,64,64,3])/255\n",
    "TIM1=[]\n",
    "for i in range(len(TIM)):\n",
    "    im = cv2.resize(TIM[i], dsize=(32,32), interpolation=cv2.INTER_CUBIC)\n",
    "    TIM1.append(im)\n",
    "TIM1 = np.array(TIM1)\n",
    "np.save(\"TIM\",TIM1)\n",
    "TIM1 = 1\n",
    "TIM = np.load(\"./TIM.npy\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.contrib.distributions import Bernoulli\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train*1./255\n",
    "x_test = x_test*1./255\n",
    "y_train = np.reshape(y_train,[50000,])\n",
    "y_test = np.reshape(y_test,[10000,])\n",
    "nb_classes = 10\n",
    "targets = y_train.reshape(-1)\n",
    "y_train = np.eye(nb_classes)[targets]\n",
    "targets = y_test.reshape(-1)\n",
    "y_test = np.eye(nb_classes)[targets]\n",
    "\n",
    "TIM = np.load(\"./TIM.npy\")\n",
    "#np.save(\"TIM1\",TIM)\n",
    "#TIMp = np.load(\"./TIM1.npy\") \n",
    "\n",
    "def crop(x):\n",
    "  for q in range(len(x)):\n",
    "    img = x[q]\n",
    "    l = []\n",
    "    for i in range(8):\n",
    "      for j in range(8):\n",
    "        im = img[i*4:(i+1)*4,j*4:(j+1)*4,:]\n",
    "        l.append(im)\n",
    "        \n",
    "    l1 = np.random.permutation(l)\n",
    "    \n",
    "    t=0\n",
    "    for i in range(8):\n",
    "      for j in range(8):\n",
    "        img[i*4:(i+1)*4,j*4:(j+1)*4,:] = l1[t]\n",
    "        t+=1\n",
    "    x[q] = img\n",
    "  return x\n",
    "\n",
    "def pad3D(c_x, padlen=1):\n",
    "    batch,m,n,r = c_x.shape\n",
    "    c_y = np.zeros((batch,m+2*padlen, n+2*padlen, r),dtype=c_x.dtype)\n",
    "    c_y[:, padlen:-padlen, padlen:-padlen,:] = c_x\n",
    "    return c_y\n",
    "\n",
    "def randomCrop(img, width, height):\n",
    "    assert img.shape[1] >= height\n",
    "    assert img.shape[2] >= width\n",
    "    x = np.random.randint(0, img.shape[2] - width)\n",
    "    y = np.random.randint(0, img.shape[1] - height)\n",
    "    img = img[:,y:y+height, x:x+width,:]\n",
    "    return img\n",
    "def stand(im):\n",
    "    im = im.astype(np.float64,copy=False)\n",
    "    mean = np.mean(im,axis=(1,2,3)).reshape(im.shape[0],1,1,1)\n",
    "    std = np.std(im,axis=(1,2,3)).reshape(im.shape[0],1,1,1)\n",
    "    std1 = std+1e-20\n",
    "    im = (im-mean)/std1\n",
    "    return im  \n",
    "  \n",
    "def stand0(im):\n",
    "  im = im.astype(np.float64,copy=False)\n",
    "  mean = np.mean(im)\n",
    "  std = np.std(im)\n",
    "  std1 = max(std,1./np.sqrt(np.array(im.size,dtype = np.float64)))\n",
    "  im = (im-mean)/std1\n",
    "  return im\n",
    "\n",
    "def stand(images):\n",
    "  for i in range(len(images)):\n",
    "    images[i] = stand0(images[i])\n",
    "  return images \n",
    "\n",
    "def stand10(safe_images):\n",
    "    safe_images = (safe_images - np.min(safe_images))/(np.max(safe_images) - np.min(safe_images))\n",
    "    return safe_images\n",
    "  \n",
    "def stand1(images):\n",
    "  for i in range(len(images)):\n",
    "    images[i] = stand10(images[i])\n",
    "  return images \n",
    "\n",
    "def random_flip(im):\n",
    "    s = np.random.randint(0,2,1)[0]\n",
    "    if s == 1:\n",
    "        im = im[:,:,::-1,...]\n",
    "    return im\n",
    "\n",
    "def preprocess(x_batch):\n",
    "    #x_batch = stand(x_batch)\n",
    "    x_batch = stand(x_batch)\n",
    "    x_batch = pad3D(x_batch, padlen=4)\n",
    "    x_batch = randomCrop(x_batch, 32, 32)\n",
    "    x_batch = random_flip(x_batch)\n",
    "    return x_batch\n",
    "\n",
    "  \n",
    "def network(img):\n",
    "        # network\n",
    "  #with tf.variable_scope('V1', reuse=tf.AUTO_REUSE):\n",
    "        con1 = tf.nn.conv2d(img, w_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv1 = tf.nn.relu(tf.layers.batch_normalization(con1, training=b))\n",
    "        h_conv1 = tf.layers.batch_normalization(tf.nn.relu(con1), training=b)\n",
    "\n",
    "        con2 = tf.nn.conv2d(h_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv2 = tf.nn.relu(tf.layers.batch_normalization(con2, training=b))\n",
    "        h_conv2 = tf.layers.batch_normalization(tf.nn.relu(con2), training=b)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        con3 = tf.nn.conv2d(h_pool2, w_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv3 = tf.nn.relu(tf.layers.batch_normalization(con3, training=b))\n",
    "        h_conv3 = tf.layers.batch_normalization(tf.nn.relu(con3), training=b)\n",
    "\n",
    "        con4 = tf.nn.conv2d(h_conv3, w_conv4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv4 = tf.nn.relu(tf.layers.batch_normalization(con4, training=b))\n",
    "        h_conv4 = tf.layers.batch_normalization(tf.nn.relu(con4), training=b)\n",
    "        h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "        con5 = tf.nn.conv2d(h_pool4, w_conv5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv5 = tf.nn.relu(tf.layers.batch_normalization(con5, training=b))\n",
    "        h_conv5 = tf.layers.batch_normalization(tf.nn.relu(con5), training=b)\n",
    "\n",
    "        con6 = tf.nn.conv2d(h_conv5, w_conv6, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv6 = tf.nn.relu(tf.layers.batch_normalization(con6, training=b))\n",
    "        h_conv6 = tf.layers.batch_normalization(tf.nn.relu(con6), training=b)\n",
    "\n",
    "        con7 = tf.nn.conv2d(h_conv6, w_conv7, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv7 = tf.nn.relu(tf.layers.batch_normalization(con7, training=b))\n",
    "        h_conv7 = tf.layers.batch_normalization(tf.nn.relu(con7), training=b)\n",
    "        h_pool7 = max_pool_2x2(h_conv7)\n",
    "\n",
    "        con8 = tf.nn.conv2d(h_pool7, w_conv8, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv8 = tf.nn.relu(tf.layers.batch_normalization(con8, training=b))\n",
    "        h_conv8 = tf.layers.batch_normalization(tf.nn.relu(con8), training=b)\n",
    "\n",
    "        con9 = tf.nn.conv2d(h_conv8, w_conv9, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv9 = tf.nn.relu(tf.layers.batch_normalization(con9, training=b))\n",
    "        h_conv9 = tf.layers.batch_normalization(tf.nn.relu(con9), training=b)\n",
    "\n",
    "        con10 = tf.nn.conv2d(h_conv9, w_conv10, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv10 = tf.nn.relu(tf.layers.batch_normalization(con10, training=b))\n",
    "        h_conv10 = tf.layers.batch_normalization(tf.nn.relu(con10), training=b)\n",
    "        h_pool10 = max_pool_2x2(h_conv10)\n",
    "\n",
    "        con11 = tf.nn.conv2d(h_pool10, w_conv11, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv11 = tf.nn.relu(tf.layers.batch_normalization(con11, training=b))\n",
    "        h_conv11 = tf.layers.batch_normalization(tf.nn.relu(con11), training=b)\n",
    "\n",
    "        con12 = tf.nn.conv2d(h_conv11, w_conv12, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv12 = tf.nn.relu(tf.layers.batch_normalization(con12, training=b))\n",
    "        h_conv12 = tf.layers.batch_normalization(tf.nn.relu(con12), training=b)\n",
    "\n",
    "        con13 = tf.nn.conv2d(h_conv12, w_conv13, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv13 = tf.nn.relu(tf.layers.batch_normalization(con13, training=b))\n",
    "        h_conv13 = tf.layers.batch_normalization(tf.nn.relu(con13), training=b)\n",
    "        h_pool13 = max_pool_2x2(h_conv13)\n",
    "\n",
    "        h_pool_flat = tf.layers.flatten(h_pool13)\n",
    "        h_pool_flat1 = tf.stack([h_pool_flat]*n_intergal_sample)\n",
    "\n",
    "        h = tf.nn.relu(tf.matmul(h_pool_flat1, w0) + b0)\n",
    "        #h = tf.nn.dropout(h,rate = 1 - 0.7)\n",
    "        h = tf.nn.relu(tf.matmul(h, w1) + b1)\n",
    "        logits = tf.matmul(h, w2) + b2\n",
    "\n",
    "        return logits#num_sample*batch*10\n",
    "def network1(img):\n",
    "        # network\n",
    "  #with tf.variable_scope('V1', reuse=tf.AUTO_REUSE):\n",
    "        con1 = tf.nn.conv2d(img, w_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv1 = tf.nn.relu(tf.layers.batch_normalization(con1, training=b))\n",
    "        h_conv1 = tf.layers.batch_normalization(tf.nn.relu(con1), training=False)\n",
    "\n",
    "        con2 = tf.nn.conv2d(h_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv2 = tf.nn.relu(tf.layers.batch_normalization(con2, training=b))\n",
    "        h_conv2 = tf.layers.batch_normalization(tf.nn.relu(con2), training=False)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        con3 = tf.nn.conv2d(h_pool2, w_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv3 = tf.nn.relu(tf.layers.batch_normalization(con3, training=b))\n",
    "        h_conv3 = tf.layers.batch_normalization(tf.nn.relu(con3), training=False)\n",
    "\n",
    "        con4 = tf.nn.conv2d(h_conv3, w_conv4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv4 = tf.nn.relu(tf.layers.batch_normalization(con4, training=b))\n",
    "        h_conv4 = tf.layers.batch_normalization(tf.nn.relu(con4), training=False)\n",
    "        h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "        con5 = tf.nn.conv2d(h_pool4, w_conv5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv5 = tf.nn.relu(tf.layers.batch_normalization(con5, training=b))\n",
    "        h_conv5 = tf.layers.batch_normalization(tf.nn.relu(con5), training=False)\n",
    "\n",
    "        con6 = tf.nn.conv2d(h_conv5, w_conv6, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv6 = tf.nn.relu(tf.layers.batch_normalization(con6, training=b))\n",
    "        h_conv6 = tf.layers.batch_normalization(tf.nn.relu(con6), training=False)\n",
    "\n",
    "        con7 = tf.nn.conv2d(h_conv6, w_conv7, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv7 = tf.nn.relu(tf.layers.batch_normalization(con7, training=b))\n",
    "        h_conv7 = tf.layers.batch_normalization(tf.nn.relu(con7), training=False)\n",
    "        h_pool7 = max_pool_2x2(h_conv7)\n",
    "\n",
    "        con8 = tf.nn.conv2d(h_pool7, w_conv8, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv8 = tf.nn.relu(tf.layers.batch_normalization(con8, training=b))\n",
    "        h_conv8 = tf.layers.batch_normalization(tf.nn.relu(con8), training=False)\n",
    "\n",
    "        con9 = tf.nn.conv2d(h_conv8, w_conv9, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv9 = tf.nn.relu(tf.layers.batch_normalization(con9, training=b))\n",
    "        h_conv9 = tf.layers.batch_normalization(tf.nn.relu(con9), training=False)\n",
    "\n",
    "        con10 = tf.nn.conv2d(h_conv9, w_conv10, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv10 = tf.nn.relu(tf.layers.batch_normalization(con10, training=b))\n",
    "        h_conv10 = tf.layers.batch_normalization(tf.nn.relu(con10), training=False)\n",
    "        h_pool10 = max_pool_2x2(h_conv10)\n",
    "\n",
    "        con11 = tf.nn.conv2d(h_pool10, w_conv11, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv11 = tf.nn.relu(tf.layers.batch_normalization(con11, training=b))\n",
    "        h_conv11 = tf.layers.batch_normalization(tf.nn.relu(con11), training=False)\n",
    "\n",
    "        con12 = tf.nn.conv2d(h_conv11, w_conv12, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv12 = tf.nn.relu(tf.layers.batch_normalization(con12, training=b))\n",
    "        h_conv12 = tf.layers.batch_normalization(tf.nn.relu(con12), training=False)\n",
    "\n",
    "        con13 = tf.nn.conv2d(h_conv12, w_conv13, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv13 = tf.nn.relu(tf.layers.batch_normalization(con13, training=b))\n",
    "        h_conv13 = tf.layers.batch_normalization(tf.nn.relu(con13), training=False)\n",
    "        h_pool13 = max_pool_2x2(h_conv13)\n",
    "\n",
    "        h_pool_flat = tf.layers.flatten(h_pool13)\n",
    "        h_pool_flat1 = tf.stack([h_pool_flat]*n_intergal_sample)\n",
    "\n",
    "        h = tf.nn.relu(tf.matmul(h_pool_flat1, w0) + b0)\n",
    "        #h = tf.nn.dropout(h,rate = 1 - 0.7)\n",
    "        h = tf.nn.relu(tf.matmul(h, w1) + b1)\n",
    "        logits = tf.matmul(h, w2) + b2\n",
    "\n",
    "        return logits#num_sample*batch*10\n",
    "\n",
    "\n",
    "M = 128\n",
    "n_intergal_sample = 600\n",
    "D=7*7*64\n",
    "h1=512\n",
    "D2=10\n",
    "max_auc = 0\n",
    "\n",
    "#for d in ['/device:GPU:0','/device:GPU:1','/device:GPU:2', '/device:GPU:3']:\n",
    "for d in ['/device:GPU:0']:\n",
    "    with tf.device(d):\n",
    "        def max_pool_2x2(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        # variables\n",
    "        noise = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "        X = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "        Y = tf.placeholder(tf.float32, [None,10])\n",
    "        b = tf.placeholder(tf.bool,shape=(),name='b')\n",
    "        learning_rate = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "\n",
    "        w_conv1 = tf.get_variable('w_conv1', [3,3,3,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv2 = tf.get_variable('w_conv2', [3,3,64,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv3 = tf.get_variable('w_conv3', [3,3,64,128], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv4 = tf.get_variable('w_conv4', [3,3,128,128], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv5 = tf.get_variable('w_conv5', [3,3,128,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv6 = tf.get_variable('w_conv6', [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv7 = tf.get_variable('w_conv7', [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv8 = tf.get_variable('w_conv8', [3,3,256,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv9 = tf.get_variable('w_conv9', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv10 = tf.get_variable('w_conv10', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv11 = tf.get_variable('w_conv11', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv12 = tf.get_variable('w_conv12', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv13 = tf.get_variable('w_conv13', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w0_m = tf.get_variable('w_fc1', [1*1*512, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b0_m = tf.get_variable('b_fc1', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w1_m = tf.get_variable('w_fc2', [1024, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1_m = tf.get_variable('b_fc2', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w2_m = tf.get_variable('w_fc3', [1024, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2_m = tf.get_variable('b_fc3', [1,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w0_r = tf.get_variable('w_fc1_r', [1*1*512, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b0_r = tf.get_variable('b_fc1_r', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w1_r = tf.get_variable('w_fc2_r', [1024, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1_r = tf.get_variable('b_fc2_r', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w2_r = tf.get_variable('w_fc3_r', [1024, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2_r = tf.get_variable('b_fc3_r', [1,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w0_sigma = w0_r**2\n",
    "        b0_sigma = b0_r**2\n",
    "        w1_sigma = w1_r**2\n",
    "        b1_sigma = b1_r**2\n",
    "        w2_sigma = w2_r**2\n",
    "        b2_sigma = b2_r**2\n",
    "\n",
    "        eps1 = tf.random_normal(shape=[n_intergal_sample,1*1*512, 1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps2 = tf.random_normal(shape=[n_intergal_sample,1,1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps3 = tf.random_normal(shape=[n_intergal_sample,1024, 1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps4 = tf.random_normal(shape=[n_intergal_sample,1,1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps5 = tf.random_normal(shape=[n_intergal_sample,1024, 10], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps6 = tf.random_normal(shape=[n_intergal_sample,1,10], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        \n",
    "        w0 = w0_m + eps1 * w0_sigma\n",
    "        b0 = b0_m + eps2 * b0_sigma\n",
    "        w1 = w1_m + eps3 * w1_sigma\n",
    "        b1 = b1_m + eps4 * b1_sigma\n",
    "        w2 = w2_m + eps5 * w2_sigma\n",
    "        b2 = b2_m + eps6 * b2_sigma\n",
    "\n",
    "        # network\n",
    "\n",
    "\n",
    "        #evaluation\n",
    "        logits0 = network(X)\n",
    "        logits = tf.reduce_mean(logits0,0)\n",
    "        output0 = tf.nn.softmax(logits0)\n",
    "        cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=Y)\n",
    "\n",
    "        probs = tf.reduce_sum(output0*Y,2)\n",
    "        prob=tf.reduce_mean(probs,0)#积分\n",
    "        log_prob = tf.log(prob+1e-30)#取log\n",
    "        #p = tf.reduce_mean(log_prob)#求和\n",
    "        p = tf.reduce_mean(-cross_ent)\n",
    "\n",
    "        output, var0 = tf.nn.moments(output0,0)#batch*10\n",
    "        prob1 = tf.reduce_sum(output*Y,1)\n",
    "        max_p = tf.reduce_max(output,1)#batch*1\n",
    "        ent = tf.reduce_sum(-tf.log(output+1e-30)*output,1)#batch*1\n",
    "        Eent = tf.reduce_mean(tf.reduce_sum(-tf.log(output0+1e-30)*output0,2),0)#batch*1\n",
    "        MI = ent - Eent#batch*1\n",
    "        MI_mean = tf.reduce_sum(MI)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(output,1), tf.argmax(Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "        #noise = tf.reshape(tf.random_normal(shape=[1,32*32*3*M], mean=0.0, stddev=1, dtype=tf.float32),[M,32,32,3])\n",
    "\n",
    "        #noise = tf.image.per_image_standardization(noise)\n",
    "\n",
    "        logits1 = network(noise)\n",
    "        output1 = tf.nn.softmax(logits1)\n",
    "        output_noise, varent_noise = tf.nn.moments(output1,0)\n",
    "        max_p_noise = tf.reduce_max(output_noise,1)\n",
    "        ent_noise = tf.reduce_sum(-tf.log(output_noise+1e-30)*output_noise,1)\n",
    "        Eent_noise = tf.reduce_mean(tf.reduce_sum(-tf.log(output1+1e-30)*output1,2),0)\n",
    "        MI_noise = ent_noise - Eent_noise\n",
    "        MI_noise_mean = tf.reduce_sum(MI_noise)\n",
    "\n",
    "        correct_pred_noise = tf.equal(tf.argmax(output_noise,1), tf.argmax(Y,1))\n",
    "        accuracy_noise = tf.reduce_mean(tf.cast(correct_pred_noise, tf.float32))\n",
    "\n",
    "        regularization = 1e-6*(tf.reduce_sum(tf.square(w0_m))+tf.reduce_sum(tf.square(b0_m))\n",
    "                               +tf.reduce_sum(tf.square(w1_m))+tf.reduce_sum(tf.square(b1_m))\n",
    "                               +tf.reduce_sum(tf.square(w2_m))+tf.reduce_sum(tf.square(b2_m)))\n",
    "                               #+tf.reduce_sum(tf.square(w_conv1))+tf.reduce_sum(tf.square(w_conv2))\n",
    "                               #+tf.reduce_sum(tf.square(w_conv3))+tf.reduce_sum(tf.square(w_conv4))\n",
    "                               #+tf.reduce_sum(tf.square(w_conv5))+tf.reduce_sum(tf.square(w_conv6))+tf.reduce_sum(tf.square(w_conv7))\n",
    "                               #+tf.reduce_sum(tf.square(w_conv8))+tf.reduce_sum(tf.square(w_conv9))+tf.reduce_sum(tf.square(w_conv10))\n",
    "                               #+tf.reduce_sum(tf.square(w_conv11))+tf.reduce_sum(tf.square(w_conv12))+tf.reduce_sum(tf.square(w_conv13)))\n",
    "        p = p*1\n",
    "        loss = -p + regularization*1 - MI_noise_mean*.7\n",
    "# training\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "with tf.Session(config = config) as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_id=13\n",
    "    save_path = saver.restore(sess, \"./density_estimation_cifar10/classification_mnist13/model.ckpt\" )\n",
    "    for i in range(100001):\n",
    "        #pp = sess.run(pop_mean1,{X: x_batch, Y: y_batch,noise:my_noise, b:False})\n",
    "        #print(0.04)\n",
    "        start_time1 = time.time()\n",
    "        seed = np.random.randint(0,50000,M)\n",
    "        x_batch = x_train[seed]\n",
    "        y_batch = y_train[seed]\n",
    "        x_batch = preprocess(x_batch)\n",
    "        \n",
    "        #pp = sess.run(MI_noise_mean,{X: my_noise, Y: y_batch,noise:my_noise, b:False})\n",
    "        #pp1 = sess.run(MI_mean,{X: my_noise, Y: y_batch,noise:my_noise, b:False})\n",
    "        #print(pp,pp1)\n",
    "        \n",
    "        #my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "        #my_noise = np.reshape(np.random.uniform(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "        #my_noise = TIMp[np.random.randint(0,10000,M)]\n",
    "        if i%3 == 0:\n",
    "          #my_noise = np.copy(x_train[np.random.randint(0,50000,M)])\n",
    "          #my_noise = crop(my_noise)\n",
    "          #my_noise = stand(my_noise)\n",
    "          my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "          \n",
    "        if i%3 == 1:\n",
    "          my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "          #my_noise = stand(my_noise)\n",
    "            \n",
    "        if i%3 == 2:\n",
    "          my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "          #my_noise = stand(my_noise)\n",
    "            \n",
    "        if i%1000==0:\n",
    "            loss_now = sess.run(loss,{X: x_batch, Y: y_batch,noise:my_noise, b:False})\n",
    "            loss_now1 = sess.run(MI_noise_mean*.7,{X: x_batch, Y: y_batch,noise:my_noise, b:False})\n",
    "            print(\"loss\",loss_now,\"-p+regular \",loss_now+loss_now1,\"MI\",loss_now1)\n",
    "            \n",
    "            train_accuracy=[]\n",
    "            seed1 = np.random.randint(0,50000,10000)\n",
    "            x_vali = x_train[seed1]\n",
    "            y_vali = y_train[seed1]\n",
    "            for j in range(0,100):\n",
    "                imgs_in = x_vali[j*100:(j+1)*100]\n",
    "                labels_in = y_vali[j*100:(j+1)*100]\n",
    "                imgs_in=stand(imgs_in)\n",
    "                acc0 = sess.run(accuracy,{X:imgs_in,Y:labels_in,noise:imgs_in,b:False})\n",
    "                train_accuracy.append(acc0)\n",
    "            train_accuracy = np.array(train_accuracy)\n",
    "            train_accuracy = np.mean(train_accuracy)\n",
    "            \n",
    "            test_accuracy=[]\n",
    "            maxp_in = []\n",
    "            ent_in = []\n",
    "            MI_in = []\n",
    "            for j in range(0,100):\n",
    "                imgs_in = x_test[j*100:(j+1)*100]\n",
    "                labels_in = y_test[j*100:(j+1)*100]\n",
    "                imgs_in=stand(imgs_in)\n",
    "                #imgs_in=stand1(imgs_in)\n",
    "                acc = sess.run(accuracy,{X:imgs_in,Y:labels_in,noise:imgs_in,b:False})\n",
    "                maxp_in1 = sess.run(max_p,{X:imgs_in,noise:imgs_in,b:False})\n",
    "                ent_in1 = sess.run(ent,{X:imgs_in,noise:imgs_in,b:False})\n",
    "                MI_in1 = sess.run(MI,{X:imgs_in,noise:imgs_in,b:False})\n",
    "                test_accuracy.append(acc)\n",
    "                maxp_in.extend(maxp_in1)\n",
    "                ent_in.extend(ent_in1)\n",
    "                MI_in.extend(MI_in1)\n",
    "            test_accuracy = np.array(test_accuracy)\n",
    "            test_accuracy = np.mean(test_accuracy)\n",
    "            maxp_in = np.array(maxp_in)\n",
    "            ent_in = np.array(ent_in)\n",
    "            MI_in = np.array(MI_in)\n",
    "            print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy,\"train accuracy\",train_accuracy)\n",
    "\n",
    "            for t in range(0,2):\n",
    "                '''\n",
    "                if t == 0:\n",
    "                    safe_images = np.reshape(np.random.normal( 0.0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                    print(\"gauss noise:\")\n",
    "                \n",
    "                if t == 1:\n",
    "                    safe_images = np.reshape(np.random.uniform(0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                    print(\"average noise:\")\n",
    "                '''\n",
    "                if t == 0:\n",
    "                    safe_images = np.copy(TIM)\n",
    "                    print(\"TIM:\")\n",
    "                if t == 1:\n",
    "                    safe_images = np.reshape(np.random.normal(0,1,[32*32*3*10000]),[10000,32,32,3])\n",
    "                    #safe_images = (safe_images - np.min(safe_images))/(np.max(safe_images) - np.min(safe_images))\n",
    "                    print(\"noise:\")\n",
    "                maxp_OOD = []\n",
    "                ent_OOD = []\n",
    "                MI_OOD = []\n",
    "                    \n",
    "                for k in range(0,100):\n",
    "                    imgs_OOD = safe_images[k*100:(k+1)*100]\n",
    "                    imgs_OOD = stand(imgs_OOD)\n",
    "                    #imgs_OOD = stand1(imgs_OOD)\n",
    "                    maxp_OOD1 = sess.run(max_p,{X:imgs_OOD,noise:imgs_OOD,b:False})\n",
    "                    ent_OOD1 = sess.run(ent,{X:imgs_OOD,noise:imgs_OOD,b:False})\n",
    "                    MI_OOD1 = sess.run(MI,{X:imgs_OOD,noise:imgs_OOD,b:False})\n",
    "                    maxp_OOD.extend(maxp_OOD1)\n",
    "                    ent_OOD.extend(ent_OOD1)\n",
    "                    MI_OOD.extend(MI_OOD1)\n",
    "                maxp_OOD = np.array(maxp_OOD)\n",
    "                ent_OOD = np.array(ent_OOD)\n",
    "                MI_OOD = np.array(MI_OOD)\n",
    "                '''\n",
    "                print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy, \"validation_error\",validation_accuracy)\n",
    "                print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "                '''\n",
    "                print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "                \n",
    "                \n",
    "                safe, risky = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "\n",
    "                safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                if t == 0:\n",
    "                    tmp_indicator = round(100*roc_auc_score(labels, examples), 2)\n",
    "\n",
    "                safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                print(\"----------------------------------------\")\n",
    "            if i==-1 and i!=0:\n",
    "              print(\"原始\") \n",
    "              train_accuracy=[]\n",
    "              seed1 = np.random.randint(0,50000,10000)\n",
    "              x_vali = x_train[seed1]\n",
    "              y_vali = y_train[seed1]\n",
    "              for j in range(0,1000):\n",
    "                  imgs_in = x_vali[j*10:(j+1)*10]\n",
    "                  labels_in = y_vali[j*10:(j+1)*10]\n",
    "                  imgs_in=stand(imgs_in)\n",
    "                  acc0 = sess.run(accuracy,{X:imgs_in,Y:labels_in,noise:imgs_in,b:True})\n",
    "                  train_accuracy.append(acc0)\n",
    "              train_accuracy = np.array(train_accuracy)\n",
    "              train_accuracy = np.mean(train_accuracy)\n",
    "\n",
    "              test_accuracy=[]\n",
    "              maxp_in = []\n",
    "              ent_in = []\n",
    "              MI_in = []\n",
    "              for j in range(0,1000):\n",
    "                  imgs_in = x_test[j*10:(j+1)*10]\n",
    "                  labels_in = y_test[j*10:(j+1)*10]\n",
    "                  imgs_in=stand(imgs_in)\n",
    "                  #imgs_in=stand1(imgs_in)\n",
    "                  acc = sess.run(accuracy,{X:imgs_in,Y:labels_in,noise:imgs_in,b:True})\n",
    "                  maxp_in1 = sess.run(max_p,{X:imgs_in,noise:imgs_in,b:True})\n",
    "                  ent_in1 = sess.run(ent,{X:imgs_in,noise:imgs_in,b:True})\n",
    "                  MI_in1 = sess.run(MI,{X:imgs_in,noise:imgs_in,b:True})\n",
    "                  test_accuracy.append(acc)\n",
    "                  maxp_in.extend(maxp_in1)\n",
    "                  ent_in.extend(ent_in1)\n",
    "                  MI_in.extend(MI_in1)\n",
    "              test_accuracy = np.array(test_accuracy)\n",
    "              test_accuracy = np.mean(test_accuracy)\n",
    "              maxp_in = np.array(maxp_in)\n",
    "              ent_in = np.array(ent_in)\n",
    "              MI_in = np.array(MI_in)\n",
    "              print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy,\"train accuracy\",train_accuracy)\n",
    "\n",
    "              for t in range(0,2):\n",
    "                  '''\n",
    "                  if t == 0:\n",
    "                      safe_images = np.reshape(np.random.normal( 0.0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                      print(\"gauss noise:\")\n",
    "\n",
    "                  if t == 1:\n",
    "                      safe_images = np.reshape(np.random.uniform(0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                      print(\"average noise:\")\n",
    "                  '''\n",
    "                  if t == 0:\n",
    "                      safe_images = TIM\n",
    "                      print(\"TIM:\")\n",
    "                  if t == 1:\n",
    "                      safe_images = np.reshape(np.random.uniform(0,1,[32*32*3*10000]),[10000,32,32,3])\n",
    "                      #safe_images = (safe_images - np.min(safe_images))/(np.max(safe_images) - np.min(safe_images))\n",
    "                      print(\"noise:\")\n",
    "                  maxp_OOD = []\n",
    "                  ent_OOD = []\n",
    "                  MI_OOD = []\n",
    "\n",
    "                  for k in range(0,1000):\n",
    "                      imgs_OOD = safe_images[k*10:(k+1)*10]\n",
    "                      imgs_OOD = stand(imgs_OOD)\n",
    "                      #imgs_OOD = stand1(imgs_OOD)\n",
    "                      maxp_OOD1 = sess.run(max_p,{X:imgs_OOD,noise:imgs_OOD,b:True})\n",
    "                      ent_OOD1 = sess.run(ent,{X:imgs_OOD,noise:imgs_OOD,b:True})\n",
    "                      MI_OOD1 = sess.run(MI,{X:imgs_OOD,noise:imgs_OOD,b:True})\n",
    "                      maxp_OOD.extend(maxp_OOD1)\n",
    "                      ent_OOD.extend(ent_OOD1)\n",
    "                      MI_OOD.extend(MI_OOD1)\n",
    "                  maxp_OOD = np.array(maxp_OOD)\n",
    "                  ent_OOD = np.array(ent_OOD)\n",
    "                  MI_OOD = np.array(MI_OOD)\n",
    "                  '''\n",
    "                  print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy, \"validation_error\",validation_accuracy)\n",
    "                  print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                  print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                  print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                  print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "                  '''\n",
    "                  print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                  print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                  print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                  print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "\n",
    "\n",
    "                  safe, risky = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[10000,1])\n",
    "                  labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                  labels[safe.shape[0]:] += 1\n",
    "                  examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                  print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "                  print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "\n",
    "                  safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "                  labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                  labels[safe.shape[0]:] += 1\n",
    "                  examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                  print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "                  print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                  if t == 0:\n",
    "                      tmp_indicator = round(100*roc_auc_score(labels, examples), 2)\n",
    "\n",
    "                  safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[10000,1])\n",
    "                  labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                  labels[safe.shape[0]:] += 1\n",
    "                  examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                  print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "                  print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                  print(\"----------------------------------------\")  \n",
    "            print(\"############################################\")\n",
    "        '''   \n",
    "        #慢    \n",
    "        if i < 10000:\n",
    "            a = 0.0001\n",
    "        elif i < 20000:\n",
    "            a = 0.00001\n",
    "        elif i < 40000:\n",
    "            a = 0.000001\n",
    "        elif i < 60000:\n",
    "            a = 0.0000001\n",
    "        elif i < 80000:\n",
    "            a = 0.00000001\n",
    "        else:\n",
    "            a = 0.000000001\n",
    "            \n",
    "        if i < 10000:\n",
    "            a = 0.0001\n",
    "        elif i < 20000:\n",
    "            a = 0.00001\n",
    "        elif i < 40000:\n",
    "            a = 0.000001\n",
    "        elif i < 60000:\n",
    "            a = 0.0000001\n",
    "        elif i < 80000:\n",
    "            a = 0.00000001\n",
    "        else:\n",
    "            a = 0.000000001\n",
    "            \n",
    "        \n",
    "            \n",
    "        #try中\n",
    "        if i < 10000:\n",
    "            a = 0.00005\n",
    "        elif i < 20000:\n",
    "            a = 0.00001\n",
    "        elif i < 30000:\n",
    "            a = 0.000003\n",
    "        elif i < 40000:\n",
    "            a = 0.000001\n",
    "        elif i < 50000:\n",
    "            a = 0.0000003\n",
    "        elif i < 60000:\n",
    "            a = 0.0000001\n",
    "        elif i < 70000:\n",
    "            a = 0.00000003\n",
    "        else:\n",
    "            a = 0.00000001\n",
    "            \n",
    "        #中  \n",
    "        if i < 5000:\n",
    "            a = 0.001\n",
    "        elif i < 10000:\n",
    "            a = 0.0003\n",
    "        elif i < 20000:\n",
    "            a = 0.0001\n",
    "        elif i < 30000:\n",
    "            a = 0.00003\n",
    "        elif i < 40000:\n",
    "            a = 0.00001\n",
    "        elif i < 50000:\n",
    "            a = 0.000003\n",
    "        elif i < 60000:\n",
    "            a = 0.000001\n",
    "        elif i < 70000:\n",
    "            a = 0.0000003\n",
    "        else:\n",
    "            a = 0.0000001\n",
    "            \n",
    "        #smooth    \n",
    "        if i < 10000:\n",
    "            a = 0.0003\n",
    "        elif i < 20000:\n",
    "            a = 0.0001\n",
    "        elif i < 30000:\n",
    "            a = 0.00003\n",
    "        elif i < 40000:\n",
    "            a = 0.00001\n",
    "        elif i < 50000:\n",
    "            a = 0.000003\n",
    "        elif i < 60000:\n",
    "            a = 0.000001\n",
    "        elif i < 70000:\n",
    "            a = 0.0000003\n",
    "        else:\n",
    "            a = 0.0000001\n",
    "        '''    \n",
    "        #快  \n",
    "        i+=13000\n",
    "        if i < 10000:\n",
    "            a = 0.001\n",
    "        elif i < 20000:\n",
    "            a = 0.0003\n",
    "        elif i < 30000:\n",
    "            a = 0.0001\n",
    "        elif i < 40000:\n",
    "            a = 0.00003\n",
    "        elif i < 50000:\n",
    "            a = 0.00001\n",
    "        elif i < 60000:\n",
    "            a = 0.000003\n",
    "        elif i < 70000:\n",
    "            a = 0.000001\n",
    "        elif i < 80000:\n",
    "            a = 0.0000003\n",
    "        else:\n",
    "            a = 0.0000001\n",
    "\n",
    "        sess.run(train_step,{X: x_batch, Y: y_batch,noise:my_noise, b:True, learning_rate:a*.2})\n",
    "        if i%1000==0:\n",
    "            save_path = saver.save(sess, \"./density_estimation_cifar10/classification_mnist%s/model.ckpt\" % model_id)\n",
    "            print(\"model\",model_id,\"saved.7\")\n",
    "            model_id+=1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.contrib.distributions import Bernoulli\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train*1./255\n",
    "x_test = x_test*1./255\n",
    "y_train = np.reshape(y_train,[50000,])\n",
    "y_test = np.reshape(y_test,[10000,])\n",
    "nb_classes = 10\n",
    "targets = y_train.reshape(-1)\n",
    "y_train = np.eye(nb_classes)[targets]\n",
    "targets = y_test.reshape(-1)\n",
    "y_test = np.eye(nb_classes)[targets]\n",
    "\n",
    "TIM = np.load(\"./TIM.npy\")  \n",
    "\n",
    "def crop(x):\n",
    "  for q in range(len(x)):\n",
    "    img = x[q]\n",
    "    l = []\n",
    "    for i in range(8):\n",
    "      for j in range(8):\n",
    "        im = img[i*4:(i+1)*4,j*4:(j+1)*4,:]\n",
    "        l.append(im)\n",
    "     \n",
    "    l1 = np.random.permutation(l)\n",
    "    \n",
    "    t=0\n",
    "    for i in range(8):\n",
    "      for j in range(8):\n",
    "        img[i*4:(i+1)*4,j*4:(j+1)*4,:] = l1[t]\n",
    "        t+=1\n",
    "    x[q] = img\n",
    "  return x\n",
    "def crop1(x):\n",
    "  for q in range(len(x)):\n",
    "    img = x[q]\n",
    "    l = []\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        im = img[i*8:(i+1)*8,j*8:(j+1)*8,:]\n",
    "        l.append(im)\n",
    "     \n",
    "    l1 = np.random.permutation(l)\n",
    "    \n",
    "    t=0\n",
    "    for i in range(4):\n",
    "      for j in range(4):\n",
    "        img[i*8:(i+1)*8,j*8:(j+1)*8,:] = l1[t]\n",
    "        t+=1\n",
    "    x[q] = img\n",
    "  return x\n",
    "\n",
    "def pad3D(c_x, padlen=1):\n",
    "    batch,m,n,r = c_x.shape\n",
    "    c_y = np.zeros((batch,m+2*padlen, n+2*padlen, r),dtype=c_x.dtype)\n",
    "    c_y[:, padlen:-padlen, padlen:-padlen,:] = c_x\n",
    "    return c_y\n",
    "\n",
    "def randomCrop(img, width, height):\n",
    "    assert img.shape[1] >= height\n",
    "    assert img.shape[2] >= width\n",
    "    x = np.random.randint(0, img.shape[2] - width)\n",
    "    y = np.random.randint(0, img.shape[1] - height)\n",
    "    img = img[:,y:y+height, x:x+width,:]\n",
    "    return img\n",
    "def stand(im):\n",
    "    im = im.astype(np.float64,copy=False)\n",
    "    mean = np.mean(im,axis=(1,2,3)).reshape(im.shape[0],1,1,1)\n",
    "    std = np.std(im,axis=(1,2,3)).reshape(im.shape[0],1,1,1)\n",
    "    std1 = std+1e-20\n",
    "    im = (im-mean)/std1\n",
    "    return im  \n",
    "  \n",
    "def stand0(im):\n",
    "  im = im.astype(np.float64,copy=False)\n",
    "  mean = np.mean(im)\n",
    "  std = np.std(im)\n",
    "  std1 = max(std,1./np.sqrt(np.array(im.size,dtype = np.float64)))\n",
    "  im = (im-mean)/std1\n",
    "  return im\n",
    "\n",
    "def stand(images):\n",
    "  for i in range(len(images)):\n",
    "    images[i] = stand0(images[i])\n",
    "  return images \n",
    "\n",
    "def stand10(safe_images):\n",
    "    safe_images = (safe_images - np.min(safe_images))/(np.max(safe_images) - np.min(safe_images))\n",
    "    return safe_images\n",
    "  \n",
    "def stand1(images):\n",
    "  for i in range(len(images)):\n",
    "    images[i] = stand10(images[i])\n",
    "  return images \n",
    "\n",
    "def random_flip(im):\n",
    "    s = np.random.randint(0,2,1)[0]\n",
    "    if s == 1:\n",
    "        im = im[:,:,::-1,...]\n",
    "    return im\n",
    "\n",
    "def preprocess(x_batch):\n",
    "    #x_batch = stand(x_batch)\n",
    "    x_batch = stand(x_batch)\n",
    "    x_batch = pad3D(x_batch, padlen=4)\n",
    "    x_batch = randomCrop(x_batch, 32, 32)\n",
    "    x_batch = random_flip(x_batch)\n",
    "    return x_batch\n",
    "\n",
    "'''\n",
    "epsilon = 1e-2\n",
    "def batch_norm(inputs, scale, beta, training, decay = 0.999):\n",
    "    \n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "    train_mean = tf.assign(pop_mean,pop_mean * decay + batch_mean * (1 - decay))\n",
    "    train_var = tf.assign(pop_var,pop_var * decay + batch_var * (1 - decay))\n",
    "\n",
    "    with tf.control_dependencies([train_mean, train_var]):\n",
    "      return (1.-training)*tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)+(training)*tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, epsilon)\n",
    "    \n",
    "    \n",
    "    if training:\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        #with tf.control_dependencies([train_mean, train_var]):\n",
    "        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\n",
    "'''\n",
    "\n",
    "epsilon = 1e-3\n",
    "          \n",
    "def batch_norm(inputs, scale, beta, training, decay = 0.99):\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs, [0,1,2])\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "          \n",
    "    def otherwise():\n",
    "        #ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "        #with tf.control_dependencies([ema_apply_op]):\n",
    "        return tf.identity(ema.average(batch_mean)), tf.identity(ema.average(batch_var))\n",
    "    \n",
    "    mean, var = tf.cond(training,mean_var_with_update,otherwise)\n",
    "    normed = tf.nn.batch_normalization(inputs, mean, var, beta, scale, 1e-3)\n",
    "    return normed\n",
    "  \n",
    "def batch_norm(inputs, scale, beta, pop_mean, pop_var,training, decay = 0.99):\n",
    "    #pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    #pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "    train_mean = tf.assign(pop_mean,pop_mean * decay + batch_mean * (1 - decay))\n",
    "    train_var = tf.assign(pop_var,pop_var * decay + batch_var * (1 - decay))\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "      with tf.control_dependencies([train_mean, train_var]):\n",
    "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "      \n",
    "    def otherwise():\n",
    "      return tf.identity(pop_mean), tf.identity(pop_var)\n",
    "\n",
    "    mean, var = tf.cond(training, mean_var_with_update, otherwise)\n",
    "    normed = tf.nn.batch_normalization(inputs, mean, var, beta, scale, 1e-3)\n",
    "    return normed\n",
    "  \n",
    "epsilon = 1e-3\n",
    "def batch_norm(inputs, scale, beta, pop_mean, pop_var,training, decay = 0.99):\n",
    "    #pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    #pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "    train_mean = tf.assign(pop_mean,pop_mean * decay + batch_mean * (1 - decay))\n",
    "    train_var = tf.assign(pop_var,pop_var * decay + batch_var * (1 - decay))\n",
    "    if training:\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        #with tf.control_dependencies([train_mean, train_var]):\n",
    "        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "def network(img, dd):\n",
    "    with tf.variable_scope('V1', reuse=tf.AUTO_REUSE):\n",
    "    #haha = tf.assign(c,cc)\n",
    "    #with tf.control_dependencies([haha]):\n",
    "        # network\n",
    "        con1 = tf.nn.conv2d(img, w_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv1 = tf.nn.relu(tf.layers.batch_normalization(con1, training=b))\n",
    "        #h_conv1 = tf.layers.batch_normalization(tf.nn.relu(con1), training=dd)\n",
    "        h_conv1 = batch_norm(tf.nn.relu(con1), scale1, beta1, pop_mean1, pop_var1,training=dd)\n",
    "        #h_conv1 = tf.nn.relu(batch_norm(con1, scale1, beta1, training=dd))\n",
    "        #h_conv1 = tf.nn.relu(con1+b_1)\n",
    "\n",
    "        con2 = tf.nn.conv2d(h_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv2 = tf.nn.relu(tf.layers.batch_normalization(con2, training=b))\n",
    "        #h_conv2 = tf.layers.batch_normalization(tf.nn.relu(con2), training=dd)\n",
    "        h_conv2 = batch_norm(tf.nn.relu(con2), scale2, beta2, pop_mean2, pop_var2,training=dd)\n",
    "        #h_conv2 = tf.nn.relu(batch_norm(con2, scale2, beta2, training=dd))\n",
    "        #h_conv2 = tf.nn.relu(con2+b_2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        con3 = tf.nn.conv2d(h_pool2, w_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv3 = tf.nn.relu(tf.layers.batch_normalization(con3, training=b))\n",
    "        #h_conv3 = tf.layers.batch_normalization(tf.nn.relu(con3), training=dd)\n",
    "        h_conv3 = batch_norm(tf.nn.relu(con3), scale3, beta3, pop_mean3, pop_var3,training=dd)\n",
    "        #h_conv3 = tf.nn.relu(batch_norm(con3, scale3, beta3, training=dd))\n",
    "        #h_conv3 = tf.nn.relu(con3+b_3)\n",
    "\n",
    "        con4 = tf.nn.conv2d(h_conv3, w_conv4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv4 = tf.nn.relu(tf.layers.batch_normalization(con4, training=b))\n",
    "        #h_conv4 = tf.layers.batch_normalization(tf.nn.relu(con4), training=dd)\n",
    "        h_conv4 = batch_norm(tf.nn.relu(con4), scale4, beta4, pop_mean4, pop_var4,training=dd)\n",
    "        #h_conv4 = tf.nn.relu(batch_norm(con4, scale4, beta4, training=dd))\n",
    "        #h_conv4 = tf.nn.relu(con4+b_4)\n",
    "        h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "        con5 = tf.nn.conv2d(h_pool4, w_conv5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv5 = tf.nn.relu(tf.layers.batch_normalization(con5, training=b))\n",
    "        #h_conv5 = tf.layers.batch_normalization(tf.nn.relu(con5), training=dd)\n",
    "        h_conv5 = batch_norm(tf.nn.relu(con5), scale5, beta5, pop_mean5, pop_var5,training=dd)\n",
    "        #h_conv5 = tf.nn.relu(batch_norm(con5, scale5, beta5, training=dd))\n",
    "        #h_conv5 = tf.nn.relu(con5+b_5)\n",
    "\n",
    "        con6 = tf.nn.conv2d(h_conv5, w_conv6, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv6 = tf.nn.relu(tf.layers.batch_normalization(con6, training=b))\n",
    "        #h_conv6 = tf.layers.batch_normalization(tf.nn.relu(con6), training=dd)\n",
    "        h_conv6 = batch_norm(tf.nn.relu(con6), scale6, beta6, pop_mean6, pop_var6,training=dd)\n",
    "        #h_conv6 = tf.nn.relu(batch_norm(con6, scale6, beta6, training=dd))\n",
    "        #h_conv6 = tf.nn.relu(con6+b_6)\n",
    "\n",
    "        con7 = tf.nn.conv2d(h_conv6, w_conv7, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv7 = tf.nn.relu(tf.layers.batch_normalization(con7, training=b))\n",
    "        #h_conv7 = tf.layers.batch_normalization(tf.nn.relu(con7), training=dd)\n",
    "        h_conv7 = batch_norm(tf.nn.relu(con7), scale7, beta7, pop_mean7, pop_var7,training=dd)\n",
    "        #h_conv7 = tf.nn.relu(batch_norm(con7, scale7, beta7, training=dd))\n",
    "        #h_conv7 = tf.nn.relu(con7+b_7)\n",
    "        h_pool7 = max_pool_2x2(h_conv7)\n",
    "\n",
    "        con8 = tf.nn.conv2d(h_pool7, w_conv8, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv8 = tf.nn.relu(tf.layers.batch_normalization(con8, training=b))\n",
    "        #h_conv8 = tf.layers.batch_normalization(tf.nn.relu(con8), training=dd)\n",
    "        h_conv8 = batch_norm(tf.nn.relu(con8), scale8, beta8, pop_mean8, pop_var8,training=dd)\n",
    "        #h_conv8 = tf.nn.relu(batch_norm(con8, scale8, beta8, training=dd))\n",
    "        #h_conv8 = tf.nn.relu(con8+b_8)\n",
    "\n",
    "        con9 = tf.nn.conv2d(h_conv8, w_conv9, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv9 = tf.nn.relu(tf.layers.batch_normalization(con9, training=b))\n",
    "        #h_conv9 = tf.layers.batch_normalization(tf.nn.relu(con9), training=dd)\n",
    "        h_conv9 = batch_norm(tf.nn.relu(con9), scale9, beta9, pop_mean9, pop_var9,training=dd)\n",
    "        #h_conv9 = tf.nn.relu(batch_norm(con9, scale9, beta9, training=dd))\n",
    "        #h_conv9 = tf.nn.relu(con9+b_9)\n",
    "\n",
    "        con10 = tf.nn.conv2d(h_conv9, w_conv10, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv10 = tf.nn.relu(tf.layers.batch_normalization(con10, training=b))\n",
    "        #h_conv10 = tf.layers.batch_normalization(tf.nn.relu(con10), training=dd)\n",
    "        h_conv10 = batch_norm(tf.nn.relu(con10), scale10, beta10, pop_mean10, pop_var10,training=dd)\n",
    "        #h_conv10 = tf.nn.relu(batch_norm(con10, scale10, beta10, training=dd))\n",
    "        #h_conv10 = tf.nn.relu(con10+b_10)\n",
    "        h_pool10 = max_pool_2x2(h_conv10)\n",
    "\n",
    "        con11 = tf.nn.conv2d(h_pool10, w_conv11, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv11 = tf.nn.relu(tf.layers.batch_normalization(con11, training=b))\n",
    "        #h_conv11 = tf.layers.batch_normalization(tf.nn.relu(con11), training=dd)\n",
    "        h_conv11 = batch_norm(tf.nn.relu(con11), scale11, beta11, pop_mean11, pop_var11,training=dd)\n",
    "        #h_conv11 = tf.nn.relu(batch_norm(con11, scale11, beta11, training=dd))\n",
    "        #h_conv11 = tf.nn.relu(con11+b_11)\n",
    "\n",
    "        con12 = tf.nn.conv2d(h_conv11, w_conv12, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv12 = tf.nn.relu(tf.layers.batch_normalization(con12, training=b))\n",
    "        #h_conv12 = tf.layers.batch_normalization(tf.nn.relu(con12), training=dd)\n",
    "        h_conv12 = batch_norm(tf.nn.relu(con12), scale12, beta12, pop_mean12, pop_var12,training=dd)\n",
    "        #h_conv12 = tf.nn.relu(batch_norm(con12, scale12, beta12, training=dd))\n",
    "        #h_conv12 = tf.nn.relu(con12+b_12)\n",
    "\n",
    "        con13 = tf.nn.conv2d(h_conv12, w_conv13, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv13 = tf.nn.relu(tf.layers.batch_normalization(con13, training=b))\n",
    "        #h_conv13 = tf.layers.batch_normalization(tf.nn.relu(con13), training=dd)\n",
    "        h_conv13 = batch_norm(tf.nn.relu(con13), scale13, beta13, pop_mean13, pop_var13,training=dd)\n",
    "        #h_conv13 = tf.nn.relu(batch_norm(con13, scale13, beta13, training=dd))\n",
    "        #h_conv13 = tf.nn.relu(con13+b_13)\n",
    "        h_pool13 = max_pool_2x2(h_conv13)\n",
    "\n",
    "        h_pool_flat = tf.layers.flatten(h_pool13)\n",
    "        h_pool_flat1 = tf.stack([h_pool_flat]*n_intergal_sample)\n",
    "\n",
    "        h = tf.nn.relu(tf.matmul(h_pool_flat1, w0) + b0)\n",
    "        #h = tf.nn.dropout(h,rate = 1 - 0.7)\n",
    "        h = tf.nn.relu(tf.matmul(h, w1) + b1)\n",
    "        logits = tf.matmul(h, w2) + b2\n",
    "\n",
    "        return logits#num_sample*batch*10\n",
    "      \n",
    "def network1(img, dd):\n",
    "    with tf.variable_scope('V1', reuse=tf.AUTO_REUSE):\n",
    "    #haha = tf.assign(c,cc)\n",
    "    #with tf.control_dependencies([haha]):\n",
    "        # network\n",
    "        con1 = tf.nn.conv2d(img, w_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv1 = tf.nn.relu(tf.layers.batch_normalization(con1, training=b))\n",
    "        #h_conv1 = tf.layers.batch_normalization(tf.nn.relu(con1), training=dd)\n",
    "        h_conv1 = batch_norm(tf.nn.relu(con1), sc1, bt1, pop_mean1, pop_var1,training=dd)\n",
    "        #h_conv1 = tf.nn.relu(batch_norm(con1, scale1, beta1, training=dd))\n",
    "        #h_conv1 = tf.nn.relu(con1+b_1)\n",
    "\n",
    "        con2 = tf.nn.conv2d(h_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv2 = tf.nn.relu(tf.layers.batch_normalization(con2, training=b))\n",
    "        #h_conv2 = tf.layers.batch_normalization(tf.nn.relu(con2), training=dd)\n",
    "        h_conv2 = batch_norm(tf.nn.relu(con2), sc2, bt2, pop_mean2, pop_var2,training=dd)\n",
    "        #h_conv2 = tf.nn.relu(batch_norm(con2, scale2, beta2, training=dd))\n",
    "        #h_conv2 = tf.nn.relu(con2+b_2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        con3 = tf.nn.conv2d(h_pool2, w_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv3 = tf.nn.relu(tf.layers.batch_normalization(con3, training=b))\n",
    "        #h_conv3 = tf.layers.batch_normalization(tf.nn.relu(con3), training=dd)\n",
    "        h_conv3 = batch_norm(tf.nn.relu(con3), sc3, bt3, pop_mean3, pop_var3,training=dd)\n",
    "        #h_conv3 = tf.nn.relu(batch_norm(con3, scale3, beta3, training=dd))\n",
    "        #h_conv3 = tf.nn.relu(con3+b_3)\n",
    "\n",
    "        con4 = tf.nn.conv2d(h_conv3, w_conv4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv4 = tf.nn.relu(tf.layers.batch_normalization(con4, training=b))\n",
    "        #h_conv4 = tf.layers.batch_normalization(tf.nn.relu(con4), training=dd)\n",
    "        h_conv4 = batch_norm(tf.nn.relu(con4), sc4, bt4, pop_mean4, pop_var4,training=dd)\n",
    "        #h_conv4 = tf.nn.relu(batch_norm(con4, scale4, beta4, training=dd))\n",
    "        #h_conv4 = tf.nn.relu(con4+b_4)\n",
    "        h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "        con5 = tf.nn.conv2d(h_pool4, w_conv5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv5 = tf.nn.relu(tf.layers.batch_normalization(con5, training=b))\n",
    "        #h_conv5 = tf.layers.batch_normalization(tf.nn.relu(con5), training=dd)\n",
    "        h_conv5 = batch_norm(tf.nn.relu(con5), sc5, bt5, pop_mean5, pop_var5,training=dd)\n",
    "        #h_conv5 = tf.nn.relu(batch_norm(con5, scale5, beta5, training=dd))\n",
    "        #h_conv5 = tf.nn.relu(con5+b_5)\n",
    "\n",
    "        con6 = tf.nn.conv2d(h_conv5, w_conv6, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv6 = tf.nn.relu(tf.layers.batch_normalization(con6, training=b))\n",
    "        #h_conv6 = tf.layers.batch_normalization(tf.nn.relu(con6), training=dd)\n",
    "        h_conv6 = batch_norm(tf.nn.relu(con6), sc6, bt6, pop_mean6, pop_var6,training=dd)\n",
    "        #h_conv6 = tf.nn.relu(batch_norm(con6, scale6, beta6, training=dd))\n",
    "        #h_conv6 = tf.nn.relu(con6+b_6)\n",
    "\n",
    "        con7 = tf.nn.conv2d(h_conv6, w_conv7, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv7 = tf.nn.relu(tf.layers.batch_normalization(con7, training=b))\n",
    "        #h_conv7 = tf.layers.batch_normalization(tf.nn.relu(con7), training=dd)\n",
    "        h_conv7 = batch_norm(tf.nn.relu(con7), sc7, bt7, pop_mean7, pop_var7,training=dd)\n",
    "        #h_conv7 = tf.nn.relu(batch_norm(con7, scale7, beta7, training=dd))\n",
    "        #h_conv7 = tf.nn.relu(con7+b_7)\n",
    "        h_pool7 = max_pool_2x2(h_conv7)\n",
    "\n",
    "        con8 = tf.nn.conv2d(h_pool7, w_conv8, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv8 = tf.nn.relu(tf.layers.batch_normalization(con8, training=b))\n",
    "        #h_conv8 = tf.layers.batch_normalization(tf.nn.relu(con8), training=dd)\n",
    "        h_conv8 = batch_norm(tf.nn.relu(con8), sc8, bt8, pop_mean8, pop_var8,training=dd)\n",
    "        #h_conv8 = tf.nn.relu(batch_norm(con8, scale8, beta8, training=dd))\n",
    "        #h_conv8 = tf.nn.relu(con8+b_8)\n",
    "\n",
    "        con9 = tf.nn.conv2d(h_conv8, w_conv9, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv9 = tf.nn.relu(tf.layers.batch_normalization(con9, training=b))\n",
    "        #h_conv9 = tf.layers.batch_normalization(tf.nn.relu(con9), training=dd)\n",
    "        h_conv9 = batch_norm(tf.nn.relu(con9), sc9, bt9, pop_mean9, pop_var9,training=dd)\n",
    "        #h_conv9 = tf.nn.relu(batch_norm(con9, scale9, beta9, training=dd))\n",
    "        #h_conv9 = tf.nn.relu(con9+b_9)\n",
    "\n",
    "        con10 = tf.nn.conv2d(h_conv9, w_conv10, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv10 = tf.nn.relu(tf.layers.batch_normalization(con10, training=b))\n",
    "        #h_conv10 = tf.layers.batch_normalization(tf.nn.relu(con10), training=dd)\n",
    "        h_conv10 = batch_norm(tf.nn.relu(con10), sc10, bt10, pop_mean10, pop_var10,training=dd)\n",
    "        #h_conv10 = tf.nn.relu(batch_norm(con10, scale10, beta10, training=dd))\n",
    "        #h_conv10 = tf.nn.relu(con10+b_10)\n",
    "        h_pool10 = max_pool_2x2(h_conv10)\n",
    "\n",
    "        con11 = tf.nn.conv2d(h_pool10, w_conv11, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv11 = tf.nn.relu(tf.layers.batch_normalization(con11, training=b))\n",
    "        #h_conv11 = tf.layers.batch_normalization(tf.nn.relu(con11), training=dd)\n",
    "        h_conv11 = batch_norm(tf.nn.relu(con11), sc11, bt11, pop_mean11, pop_var11,training=dd)\n",
    "        #h_conv11 = tf.nn.relu(batch_norm(con11, scale11, beta11, training=dd))\n",
    "        #h_conv11 = tf.nn.relu(con11+b_11)\n",
    "\n",
    "        con12 = tf.nn.conv2d(h_conv11, w_conv12, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv12 = tf.nn.relu(tf.layers.batch_normalization(con12, training=b))\n",
    "        #h_conv12 = tf.layers.batch_normalization(tf.nn.relu(con12), training=dd)\n",
    "        h_conv12 = batch_norm(tf.nn.relu(con12), sc12, bt12, pop_mean12, pop_var12,training=dd)\n",
    "        #h_conv12 = tf.nn.relu(batch_norm(con12, scale12, beta12, training=dd))\n",
    "        #h_conv12 = tf.nn.relu(con12+b_12)\n",
    "\n",
    "        con13 = tf.nn.conv2d(h_conv12, w_conv13, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv13 = tf.nn.relu(tf.layers.batch_normalization(con13, training=b))\n",
    "        #h_conv13 = tf.layers.batch_normalization(tf.nn.relu(con13), training=dd)\n",
    "        h_conv13 = batch_norm(tf.nn.relu(con13), sc13, bt13, pop_mean13, pop_var13,training=dd)\n",
    "        #h_conv13 = tf.nn.relu(batch_norm(con13, scale13, beta13, training=dd))\n",
    "        #h_conv13 = tf.nn.relu(con13+b_13)\n",
    "        h_pool13 = max_pool_2x2(h_conv13)\n",
    "\n",
    "        h_pool_flat = tf.layers.flatten(h_pool13)\n",
    "        h_pool_flat1 = tf.stack([h_pool_flat]*n_intergal_sample)\n",
    "\n",
    "        h = tf.nn.relu(tf.matmul(h_pool_flat1, w0) + b0)\n",
    "        #h = tf.nn.dropout(h,rate = 1 - 0.7)\n",
    "        h = tf.nn.relu(tf.matmul(h, w1) + b1)\n",
    "        logits = tf.matmul(h, w2) + b2\n",
    "\n",
    "        return logits#num_sample*batch*10\n",
    "\n",
    "M = 128\n",
    "n_intergal_sample = 30\n",
    "D=7*7*64\n",
    "h1=512\n",
    "D2=10\n",
    "max_auc = 0\n",
    "\n",
    "#for d in ['/device:GPU:0','/device:GPU:1','/device:GPU:2', '/device:GPU:3']:\n",
    "for d in ['/device:GPU:0']:\n",
    "    with tf.device(d):\n",
    "        def max_pool_2x2(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        # variables\n",
    "        noise = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "        X = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "        Y = tf.placeholder(tf.float32, [None,10])\n",
    "        b = tf.placeholder(tf.bool,shape=(),name='b')\n",
    "        #c = tf.Variable(False, trainable=False)\n",
    "        learning_rate = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "\n",
    "        pop_var1 = tf.Variable(tf.ones([64]), trainable=False)\n",
    "        pop_mean1 = tf.Variable(tf.zeros([64]), trainable=False)\n",
    "        pop_var2 = tf.Variable(tf.ones([64]), trainable=False)\n",
    "        pop_mean2 = tf.Variable(tf.zeros([64]), trainable=False)\n",
    "        pop_var3 = tf.Variable(tf.ones([128]), trainable=False)\n",
    "        pop_mean3 = tf.Variable(tf.zeros([128]), trainable=False)\n",
    "        pop_var4 = tf.Variable(tf.ones([128]), trainable=False)\n",
    "        pop_mean4 = tf.Variable(tf.zeros([128]), trainable=False)\n",
    "        pop_var5 = tf.Variable(tf.ones([256]), trainable=False)\n",
    "        pop_mean5 = tf.Variable(tf.zeros([256]), trainable=False)\n",
    "        pop_var6 = tf.Variable(tf.ones([256]), trainable=False)\n",
    "        pop_mean6 = tf.Variable(tf.zeros([256]), trainable=False)\n",
    "        pop_var7 = tf.Variable(tf.ones([256]), trainable=False)\n",
    "        pop_mean7 = tf.Variable(tf.zeros([256]), trainable=False)\n",
    "        pop_var8 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        pop_mean8 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        pop_var9 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        pop_mean9 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        pop_var10 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        pop_mean10 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        pop_var11 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        pop_mean11 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        pop_var12 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        pop_mean12 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        pop_var13 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        pop_mean13 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        \n",
    "        scale1 = tf.Variable(tf.ones([64]))\n",
    "        beta1 = tf.Variable(tf.zeros([64]))\n",
    "        scale2 = tf.Variable(tf.ones([64]))\n",
    "        beta2 = tf.Variable(tf.zeros([64]))\n",
    "        scale3 = tf.Variable(tf.ones([128]))\n",
    "        beta3 = tf.Variable(tf.zeros([128]))\n",
    "        scale4 = tf.Variable(tf.ones([128]))\n",
    "        beta4 = tf.Variable(tf.zeros([128]))\n",
    "        scale5 = tf.Variable(tf.ones([256]))\n",
    "        beta5 = tf.Variable(tf.zeros([256]))\n",
    "        scale6 = tf.Variable(tf.ones([256]))\n",
    "        beta6 = tf.Variable(tf.zeros([256]))\n",
    "        scale7 = tf.Variable(tf.ones([256]))\n",
    "        beta7 = tf.Variable(tf.zeros([256]))\n",
    "        scale8 = tf.Variable(tf.ones([512]))\n",
    "        beta8 = tf.Variable(tf.zeros([512]))\n",
    "        scale9 = tf.Variable(tf.ones([512]))\n",
    "        beta9 = tf.Variable(tf.zeros([512]))\n",
    "        scale10 = tf.Variable(tf.ones([512]))\n",
    "        beta10 = tf.Variable(tf.zeros([512]))\n",
    "        scale11 = tf.Variable(tf.ones([512]))\n",
    "        beta11 = tf.Variable(tf.zeros([512]))\n",
    "        scale12 = tf.Variable(tf.ones([512]))\n",
    "        beta12 = tf.Variable(tf.zeros([512]))\n",
    "        scale13 = tf.Variable(tf.ones([512]))\n",
    "        beta13 = tf.Variable(tf.zeros([512]))\n",
    "        \n",
    "        bt1 = tf.stop_gradient(tf.identity(beta1))\n",
    "        sc1 = tf.stop_gradient(tf.identity(scale1))\n",
    "        bt2 = tf.stop_gradient(tf.identity(beta2))\n",
    "        sc2 = tf.stop_gradient(tf.identity(scale2))\n",
    "        bt3 = tf.stop_gradient(tf.identity(beta3))\n",
    "        sc3 = tf.stop_gradient(tf.identity(scale3))\n",
    "        bt4 = tf.stop_gradient(tf.identity(beta4))\n",
    "        sc4 = tf.stop_gradient(tf.identity(scale4))\n",
    "        bt5 = tf.stop_gradient(tf.identity(beta5))\n",
    "        sc5 = tf.stop_gradient(tf.identity(scale5))\n",
    "        bt6 = tf.stop_gradient(tf.identity(beta6))\n",
    "        sc6 = tf.stop_gradient(tf.identity(scale6))        \n",
    "        bt7 = tf.stop_gradient(tf.identity(beta7))\n",
    "        sc7 = tf.stop_gradient(tf.identity(scale7))        \n",
    "        bt8 = tf.stop_gradient(tf.identity(beta8))\n",
    "        sc8 = tf.stop_gradient(tf.identity(scale8))        \n",
    "        bt9 = tf.stop_gradient(tf.identity(beta9))\n",
    "        sc9 = tf.stop_gradient(tf.identity(scale9))\n",
    "        bt10 = tf.stop_gradient(tf.identity(beta10))\n",
    "        sc10 = tf.stop_gradient(tf.identity(scale10))        \n",
    "        bt11 = tf.stop_gradient(tf.identity(beta11))\n",
    "        sc11 = tf.stop_gradient(tf.identity(scale11))\n",
    "        bt12 = tf.stop_gradient(tf.identity(beta12))\n",
    "        sc12 = tf.stop_gradient(tf.identity(scale12))        \n",
    "        bt13 = tf.stop_gradient(tf.identity(beta13))\n",
    "        sc13 = tf.stop_gradient(tf.identity(scale13))\n",
    "        \n",
    "        w_conv1 = tf.get_variable('w_conv1', [3,3,3,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv2 = tf.get_variable('w_conv2', [3,3,64,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv3 = tf.get_variable('w_conv3', [3,3,64,128], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv4 = tf.get_variable('w_conv4', [3,3,128,128], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv5 = tf.get_variable('w_conv5', [3,3,128,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv6 = tf.get_variable('w_conv6', [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv7 = tf.get_variable('w_conv7', [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv8 = tf.get_variable('w_conv8', [3,3,256,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv9 = tf.get_variable('w_conv9', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv10 = tf.get_variable('w_conv10', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv11 = tf.get_variable('w_conv11', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv12 = tf.get_variable('w_conv12', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv13 = tf.get_variable('w_conv13', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w0_m = tf.get_variable('w_fc1', [1*1*512, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b0_m = tf.get_variable('b_fc1', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w1_m = tf.get_variable('w_fc2', [1024, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1_m = tf.get_variable('b_fc2', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w2_m = tf.get_variable('w_fc3', [1024, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2_m = tf.get_variable('b_fc3', [1,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        w0_r = tf.get_variable('w_fc1_r', [1*1*512, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b0_r = tf.get_variable('b_fc1_r', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w1_r = tf.get_variable('w_fc2_r', [1024, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1_r = tf.get_variable('b_fc2_r', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w2_r = tf.get_variable('w_fc3_r', [1024, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2_r = tf.get_variable('b_fc3_r', [1,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w0_sigma = w0_r**2\n",
    "        b0_sigma = b0_r**2\n",
    "        w1_sigma = w1_r**2\n",
    "        b1_sigma = b1_r**2\n",
    "        w2_sigma = w2_r**2\n",
    "        b2_sigma = b2_r**2\n",
    "\n",
    "        eps1 = tf.random_normal(shape=[n_intergal_sample,1*1*512, 1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps2 = tf.random_normal(shape=[n_intergal_sample,1,1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps3 = tf.random_normal(shape=[n_intergal_sample,1024, 1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps4 = tf.random_normal(shape=[n_intergal_sample,1,1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps5 = tf.random_normal(shape=[n_intergal_sample,1024, 10], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps6 = tf.random_normal(shape=[n_intergal_sample,1,10], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "\n",
    "        w0 = w0_m + eps1 * w0_sigma\n",
    "        b0 = b0_m + eps2 * b0_sigma\n",
    "        w1 = w1_m + eps3 * w1_sigma\n",
    "        b1 = b1_m + eps4 * b1_sigma\n",
    "        w2 = w2_m + eps5 * w2_sigma\n",
    "        b2 = b2_m + eps6 * b2_sigma\n",
    "\n",
    "        # network\n",
    "\n",
    "\n",
    "        #evaluation\n",
    "        ###训练接口\n",
    "        logits0 = network(X,True)\n",
    "        logits = tf.reduce_mean(logits0,0)\n",
    "        output0 = tf.nn.softmax(logits0)\n",
    "        cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=Y)\n",
    "        \n",
    "        probs = tf.reduce_sum(output0*Y,2)\n",
    "        prob=tf.reduce_mean(probs,0)#积分\n",
    "        log_prob = tf.log(prob+1e-30)#取log\n",
    "        #p = tf.reduce_mean(log_prob)#求和\n",
    "        p = tf.reduce_mean(-cross_ent)\n",
    "\n",
    "        output, var0 = tf.nn.moments(output0,0)#batch*10\n",
    "        prob1 = tf.reduce_sum(output*Y,1)\n",
    "        max_p = tf.reduce_max(output,1)\n",
    "        ent = tf.reduce_sum(-tf.log(output+1e-30)*output,1)\n",
    "        Eent = tf.reduce_mean(tf.reduce_sum(-tf.log(output0+1e-30)*output0,2),0)\n",
    "        MI = ent - Eent\n",
    "        MI_mean = tf.reduce_sum(MI)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(output,1), tf.argmax(Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        \n",
    "        ###测试接口\n",
    "        logits2 = network(X,False)\n",
    "        logits22 = tf.reduce_mean(logits2,0)\n",
    "        cross_ent_test = tf.nn.softmax_cross_entropy_with_logits(logits = logits22, labels=Y)\n",
    "        p_test = tf.reduce_mean(-cross_ent_test)\n",
    "        \n",
    "        output2 = tf.nn.softmax(logits2)\n",
    "        output_test, var_test = tf.nn.moments(output2,0)#batch*10\n",
    "        max_p_test = tf.reduce_max(output_test,1)\n",
    "        ent_test = tf.reduce_sum(-tf.log(output_test+1e-30)*output_test,1)\n",
    "        Eent_test = tf.reduce_mean(tf.reduce_sum(-tf.log(output2+1e-30)*output2,2),0)\n",
    "        MI_test = ent_test - Eent_test\n",
    "        MI_mean_test = tf.reduce_sum(MI_test)\n",
    "\n",
    "        correct_pred_test = tf.equal(tf.argmax(output_test,1), tf.argmax(Y,1))\n",
    "        accuracy_test = tf.reduce_mean(tf.cast(correct_pred_test, tf.float32))\n",
    "\n",
    "        ##noise接口\n",
    "        logits1 = network1(noise,False)\n",
    "        output1 = tf.nn.softmax(logits1)\n",
    "        output_noise, varent_noise = tf.nn.moments(output1,0)\n",
    "        max_p_noise = tf.reduce_max(output_noise,1)\n",
    "        ent_noise = tf.reduce_sum(-tf.log(output_noise+1e-30)*output_noise,1)\n",
    "        Eent_noise = tf.reduce_mean(tf.reduce_sum(-tf.log(output1+1e-30)*output1,2),0)\n",
    "        MI_noise = ent_noise - Eent_noise\n",
    "        MI_noise_mean = tf.reduce_sum(MI_noise)\n",
    "        \n",
    "        correct_pred_noise = tf.equal(tf.argmax(output_noise,1), tf.argmax(Y,1))\n",
    "        accuracy_noise = tf.reduce_mean(tf.cast(correct_pred_noise, tf.float32))\n",
    "\n",
    "        regularization = 1e-4*(tf.reduce_sum(tf.square(w0_m))+tf.reduce_sum(tf.square(b0_m))\n",
    "                               +tf.reduce_sum(tf.square(w1_m))+tf.reduce_sum(tf.square(b1_m))\n",
    "                               +tf.reduce_sum(tf.square(w2_m))+tf.reduce_sum(tf.square(b2_m))\n",
    "                               +tf.reduce_sum(tf.square(w_conv1))+tf.reduce_sum(tf.square(w_conv2))\n",
    "                               +tf.reduce_sum(tf.square(w_conv3))+tf.reduce_sum(tf.square(w_conv4))\n",
    "                               +tf.reduce_sum(tf.square(w_conv5))+tf.reduce_sum(tf.square(w_conv6))+tf.reduce_sum(tf.square(w_conv7))\n",
    "                               +tf.reduce_sum(tf.square(w_conv8))+tf.reduce_sum(tf.square(w_conv9))+tf.reduce_sum(tf.square(w_conv10))\n",
    "                               +tf.reduce_sum(tf.square(w_conv11))+tf.reduce_sum(tf.square(w_conv12))+tf.reduce_sum(tf.square(w_conv13)))\n",
    "        \n",
    "        regu = 0.*(tf.reduce_sum(tf.square(w0_r))+tf.reduce_sum(tf.square(b0_r))\n",
    "                   +tf.reduce_sum(tf.square(w1_r))+tf.reduce_sum(tf.square(b1_r))\n",
    "                   +tf.reduce_sum(tf.square(w2_r))+tf.reduce_sum(tf.square(b2_r)))\n",
    "\n",
    "        loss = -p*0 + regularization - MI_noise_mean*0.4 - regu\n",
    "        loss_test = -p_test + regularization - MI_noise_mean*0.4 - regu\n",
    "# training\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "with tf.Session(config = config) as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_id=0\n",
    "    #save_path = saver.restore(sess, \"./density_estimation_cifar10/classification_mnist17/model.ckpt\" )\n",
    "    for i in range(180000):\n",
    "        start_time1 = time.time()\n",
    "        seed = np.random.randint(0,50000,M)\n",
    "        x_batch = x_train[seed]\n",
    "        y_batch = y_train[seed]\n",
    "        x_batch = preprocess(x_batch)\n",
    "        '''\n",
    "        for ind in range(len(TIM)):\n",
    "          mp = sess.run(max_p_test,{X:TIM[ind].reshape([1,32,32,3])})\n",
    "          lb = sess.run(output_test,{X:TIM[ind].reshape([1,32,32,3])})\n",
    "          #print(ind,mp,TIM[ind].mean(),TIM[ind].std())\n",
    "          if mp > 0.96:\n",
    "            print(\"this is abnormal: \", ind,mp,TIM[ind].mean(),TIM[ind].std())\n",
    "            print(lb)\n",
    "            #plt.imshow(TIM[ind])\n",
    "            #plt.show()   \n",
    "        '''\n",
    "        \n",
    "        #my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "        #my_noise = np.reshape(np.random.uniform(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "        #my_noise = stand(my_noise)\n",
    "        \n",
    "        if i%3 == 0:\n",
    "          my_noise = np.copy(x_train[np.random.randint(0,50000,M)])\n",
    "          my_noise = crop(my_noise)\n",
    "          my_noise = stand(my_noise)\n",
    "          \n",
    "        if i%3 == 1:\n",
    "          my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "          \n",
    "        if i%3 == 2:\n",
    "          my_noise = np.reshape(np.random.uniform(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "           \n",
    "        if i%1000==0:\n",
    "            #pp = sess.run(pop_mean1,{X: my_noise, Y: y_batch,noise:my_noise, b:False})\n",
    "            #pp1 = sess.run(pop_var1,{X: my_noise, Y: y_batch,noise:my_noise, b:False})\n",
    "            #print(pp[0],pp1[0])\n",
    "\n",
    "            loss_now = sess.run(loss_test,{X: x_batch, Y: y_batch,noise:my_noise, b:False})\n",
    "            loss_now1 = sess.run(MI_noise_mean*0.4,{X: x_batch, Y: y_batch,noise:my_noise, b:False})\n",
    "            #loss_now2 = sess.run(regu,{X: x_batch, Y: y_batch,noise:my_noise, b:False})\n",
    "            print(\"loss\",loss_now,\"-p+regular \",loss_now+loss_now1,\"MI\",loss_now1)\n",
    "            train_accuracy=[]\n",
    "            \n",
    "            seed1 = np.random.randint(0,50000,10000)\n",
    "            x_vali = x_train[seed1]\n",
    "            y_vali = y_train[seed1]\n",
    "            for j in range(0,10):\n",
    "                imgs_in = x_vali[j*1000:(j+1)*1000]\n",
    "                labels_in = y_vali[j*1000:(j+1)*1000]\n",
    "                imgs_in=stand(imgs_in)\n",
    "                acc0 = sess.run(accuracy_test,{X:imgs_in,Y:labels_in,noise:imgs_in,b:False})\n",
    "                train_accuracy.append(acc0)\n",
    "            train_accuracy = np.array(train_accuracy)\n",
    "            train_accuracy = np.mean(train_accuracy)\n",
    "            \n",
    "            test_accuracy=[]\n",
    "            maxp_in = []\n",
    "            ent_in = []\n",
    "            MI_in = []\n",
    "            for j in range(0,100):\n",
    "                imgs_in = x_test[j*100:(j+1)*100]\n",
    "                labels_in = y_test[j*100:(j+1)*100]\n",
    "                imgs_in=stand(imgs_in)\n",
    "                #imgs_in=stand1(imgs_in)\n",
    "                acc = sess.run(accuracy_test,{X:imgs_in,Y:labels_in,noise:imgs_in,b:False})\n",
    "                maxp_in1 = sess.run(max_p_test,{X:imgs_in,noise:imgs_in,b:False})\n",
    "                ent_in1 = sess.run(ent_test,{X:imgs_in,noise:imgs_in,b:False})\n",
    "                MI_in1 = sess.run(MI_test,{X:imgs_in,noise:imgs_in,b:False})\n",
    "                test_accuracy.append(acc)\n",
    "                maxp_in.extend(maxp_in1)\n",
    "                ent_in.extend(ent_in1)\n",
    "                MI_in.extend(MI_in1)\n",
    "            test_accuracy = np.array(test_accuracy)\n",
    "            test_accuracy = np.mean(test_accuracy)\n",
    "            maxp_in = np.array(maxp_in)\n",
    "            ent_in = np.array(ent_in)\n",
    "            MI_in = np.array(MI_in)\n",
    "            print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy,\"train accuracy\",train_accuracy)\n",
    "\n",
    "            for t in range(0,2):\n",
    "                '''\n",
    "                if t == 0:\n",
    "                    safe_images = np.reshape(np.random.normal( 0.0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                    print(\"gauss noise:\")\n",
    "                \n",
    "                if t == 1:\n",
    "                    safe_images = np.reshape(np.random.uniform(0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                    print(\"average noise:\")\n",
    "                '''\n",
    "                if t == 0:\n",
    "                    safe_images = TIM\n",
    "                    print(\"TIM:\")\n",
    "                if t == 1:\n",
    "                    safe_images = np.reshape(np.random.uniform(0,1,[32*32*3*10000]),[10000,32,32,3])\n",
    "                    #safe_images = x_train[np.random.randint(0,50000,10000)]\n",
    "                    #safe_images = crop(safe_images)\n",
    "                    print(\"noise:\")\n",
    "                maxp_OOD = []\n",
    "                ent_OOD = []\n",
    "                MI_OOD = []\n",
    "                    \n",
    "                for k in range(0,100):\n",
    "                    imgs_OOD = safe_images[k*100:(k+1)*100]\n",
    "                    imgs_OOD = stand(imgs_OOD)\n",
    "                    #imgs_OOD = stand1(imgs_OOD)\n",
    "                    maxp_OOD1 = sess.run(max_p_test,{X:imgs_OOD,noise:imgs_OOD,b:False})\n",
    "                    ent_OOD1 = sess.run(ent_test,{X:imgs_OOD,noise:imgs_OOD,b:False})\n",
    "                    MI_OOD1 = sess.run(MI_test,{X:imgs_OOD,noise:imgs_OOD,b:False})\n",
    "                    maxp_OOD.extend(maxp_OOD1)\n",
    "                    ent_OOD.extend(ent_OOD1)\n",
    "                    MI_OOD.extend(MI_OOD1)\n",
    "                maxp_OOD = np.array(maxp_OOD)\n",
    "                ent_OOD = np.array(ent_OOD)\n",
    "                MI_OOD = np.array(MI_OOD)\n",
    "                '''\n",
    "                print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy, \"validation_error\",validation_accuracy)\n",
    "                print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "                '''\n",
    "                print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "                \n",
    "                \n",
    "                safe, risky = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "\n",
    "                safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                if t == 0:\n",
    "                    tmp_indicator = round(100*roc_auc_score(labels, examples), 2)\n",
    "\n",
    "                safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                print(\"----------------------------------------\")\n",
    "               \n",
    "                \n",
    "            print(\"############################################\")\n",
    "        '''   \n",
    "        #慢    \n",
    "        if i < 10000:\n",
    "            a = 0.0001\n",
    "        elif i < 20000:\n",
    "            a = 0.00001\n",
    "        elif i < 40000:\n",
    "            a = 0.000001\n",
    "        elif i < 60000:\n",
    "            a = 0.0000001\n",
    "        elif i < 80000:\n",
    "            a = 0.00000001\n",
    "        else:\n",
    "            a = 0.000000001\n",
    "            \n",
    "        if i < 10000:\n",
    "            a = 0.0001\n",
    "        elif i < 20000:\n",
    "            a = 0.00001\n",
    "        elif i < 40000:\n",
    "            a = 0.000001\n",
    "        elif i < 60000:\n",
    "            a = 0.0000001\n",
    "        elif i < 80000:\n",
    "            a = 0.00000001\n",
    "        else:\n",
    "            a = 0.000000001\n",
    "            \n",
    "        \n",
    "            \n",
    "        #try中\n",
    "        if i < 10000:\n",
    "            a = 0.00005\n",
    "        elif i < 20000:\n",
    "            a = 0.00001\n",
    "        elif i < 30000:\n",
    "            a = 0.000003\n",
    "        elif i < 40000:\n",
    "            a = 0.000001\n",
    "        elif i < 50000:\n",
    "            a = 0.0000003\n",
    "        elif i < 60000:\n",
    "            a = 0.0000001\n",
    "        elif i < 70000:\n",
    "            a = 0.00000003\n",
    "        else:\n",
    "            a = 0.00000001\n",
    "            \n",
    "        #中  \n",
    "        if i < 5000:\n",
    "            a = 0.001\n",
    "        elif i < 10000:\n",
    "            a = 0.0003\n",
    "        elif i < 20000:\n",
    "            a = 0.0001\n",
    "        elif i < 30000:\n",
    "            a = 0.00003\n",
    "        elif i < 40000:\n",
    "            a = 0.00001\n",
    "        elif i < 50000:\n",
    "            a = 0.000003\n",
    "        elif i < 60000:\n",
    "            a = 0.000001\n",
    "        elif i < 70000:\n",
    "            a = 0.0000003\n",
    "        else:\n",
    "            a = 0.0000001\n",
    "            \n",
    "        #smooth    \n",
    "        if i < 10000:\n",
    "            a = 0.0003\n",
    "        elif i < 20000:\n",
    "            a = 0.0001\n",
    "        elif i < 30000:\n",
    "            a = 0.00003\n",
    "        elif i < 40000:\n",
    "            a = 0.00001\n",
    "        elif i < 50000:\n",
    "            a = 0.000003\n",
    "        elif i < 60000:\n",
    "            a = 0.000001\n",
    "        elif i < 70000:\n",
    "            a = 0.0000003\n",
    "        else:\n",
    "            a = 0.0000001\n",
    "        '''    \n",
    "        #快   \n",
    "        if i < 10000:\n",
    "            a = 0.001\n",
    "        elif i < 20000:\n",
    "            a = 0.0003\n",
    "        elif i < 30000:\n",
    "            a = 0.0001\n",
    "        elif i < 40000:\n",
    "            a = 0.00003\n",
    "        elif i < 50000:\n",
    "            a = 0.00001\n",
    "        elif i < 60000:\n",
    "            a = 0.000003\n",
    "        elif i < 70000:\n",
    "            a = 0.000001\n",
    "        elif i < 80000:\n",
    "            a = 0.0000003\n",
    "        else:\n",
    "            a = 0.0000001\n",
    "\n",
    "        sess.run(train_step,{X: x_batch, Y: y_batch,noise:my_noise, b:True, learning_rate:a*0.2})\n",
    "\n",
    "        if i%1000==0:\n",
    "            save_path = saver.save(sess, \"./density_estimation_cifar10/classification_mnist%s/model.ckpt\" % model_id)\n",
    "            print(\"model\",model_id,\"savedp\")\n",
    "            model_id+=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def f(theta,y,n):\n",
    "    return (y/theta+(n-y)/(1-theta))/(1-theta)**(n-y)\n",
    "\n",
    "def f1(theta,y,n,a,b):\n",
    "    return theta**(y+a-1)*(1-theta)**(n-y+b-1)\n",
    "\n",
    "theta = np.linspace(0,1)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/senqicao/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(theta,f(theta,1,2))\n",
    "#plt.plot(theta,f1(theta,1,1,.5,.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.image as mpimg\n",
    "from tensorflow.contrib.distributions import Bernoulli\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train*1./255\n",
    "x_test = x_test*1./255\n",
    "y_train = np.reshape(y_train,[50000,])\n",
    "y_test = np.reshape(y_test,[10000,])\n",
    "nb_classes = 10\n",
    "targets = y_train.reshape(-1)\n",
    "y_train = np.eye(nb_classes)[targets]\n",
    "targets = y_test.reshape(-1)\n",
    "y_test = np.eye(nb_classes)[targets]\n",
    "\n",
    "TIM = np.load(\"./TIM.npy\")  \n",
    "\n",
    "def pad3D(c_x, padlen=1):\n",
    "    batch,m,n,r = c_x.shape\n",
    "    c_y = np.zeros((batch,m+2*padlen, n+2*padlen, r),dtype=c_x.dtype)\n",
    "    c_y[:, padlen:-padlen, padlen:-padlen,:] = c_x\n",
    "    return c_y\n",
    "\n",
    "def randomCrop(img, width, height):\n",
    "    assert img.shape[1] >= height\n",
    "    assert img.shape[2] >= width\n",
    "    x = np.random.randint(0, img.shape[2] - width)\n",
    "    y = np.random.randint(0, img.shape[1] - height)\n",
    "    img = img[:,y:y+height, x:x+width,:]\n",
    "    return img\n",
    "\n",
    "def stand0(im):\n",
    "    im = im.astype(np.float64,copy=False)\n",
    "    mean = np.mean(im)\n",
    "    std = np.std(im)\n",
    "    std1 = max(std,1./np.sqrt(np.array(im.size,dtype = np.float64)))\n",
    "    im = (im-mean)/std1\n",
    "    return im\n",
    "  \n",
    "def stand(images):\n",
    "  for i in range(len(images)):\n",
    "    images[i] = stand0(images[i])\n",
    "  return images \n",
    "\n",
    "  \n",
    "def stand10(safe_images):\n",
    "    safe_images = (safe_images - np.min(safe_images))/(np.max(safe_images) - np.min(safe_images))\n",
    "    return safe_images\n",
    "  \n",
    "def stand1(images):\n",
    "  for i in range(len(images)):\n",
    "    images[i] = stand10(images[i])\n",
    "  return images \n",
    "\n",
    "def random_flip(im):\n",
    "    s = np.random.randint(0,2,1)[0]\n",
    "    if s == 1:\n",
    "        im = im[:,:,::-1,...]\n",
    "    return im\n",
    "\n",
    "def preprocess(x_batch):\n",
    "    x_batch = stand(x_batch)\n",
    "    #x_batch = stand1(x_batch)\n",
    "    x_batch = pad3D(x_batch, padlen=4)\n",
    "    x_batch = randomCrop(x_batch, 32, 32)\n",
    "    x_batch = random_flip(x_batch)\n",
    "    return x_batch\n",
    "'''\n",
    "epsilon = 1e-3\n",
    "def batch_norm(inputs, scale, beta, training, decay = 0.99):\n",
    "    \n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "    train_mean = tf.assign(pop_mean,pop_mean * decay + batch_mean * (1 - decay))\n",
    "    train_var = tf.assign(pop_var,pop_var * decay + batch_var * (1 - decay))\n",
    "    if training:\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\n",
    "          \n",
    "def batch_norm(inputs, scale, beta, training, decay = 0.99):\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs, [0,1,2])\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "          \n",
    "    def otherwise():\n",
    "        ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(ema.average(batch_mean)), tf.identity(ema.average(batch_var))\n",
    "    \n",
    "    mean, var = tf.cond(training,mean_var_with_update,otherwise)\n",
    "    normed = tf.nn.batch_normalization(inputs, mean, var, beta, scale, 1e-3)\n",
    "    return normed\n",
    "'''\n",
    "epsilon = 1e-3\n",
    "def batch_norm(inputs, scale, beta, training, decay = 0.99):\n",
    "    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n",
    "    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2])\n",
    "    train_mean = tf.assign(pop_mean,pop_mean * decay + batch_mean * (1 - decay))\n",
    "    train_var = tf.assign(pop_var,pop_var * decay + batch_var * (1 - decay))\n",
    "    if training:\n",
    "        with tf.control_dependencies([train_mean, train_var]):\n",
    "            return tf.nn.batch_normalization(inputs, batch_mean, batch_var, beta, scale, epsilon)\n",
    "    else:\n",
    "        #with tf.control_dependencies([train_mean, train_var]):\n",
    "        return tf.nn.batch_normalization(inputs, pop_mean, pop_var, beta, scale, epsilon)\n",
    "\n",
    "def network(img,dd):\n",
    "        # network\n",
    "    with tf.variable_scope('V1', reuse=tf.AUTO_REUSE):\n",
    "        con1 = tf.nn.conv2d(img, w_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv1 = tf.nn.relu(tf.layers.batch_normalization(con1, training=b))\n",
    "        #h_conv1 = tf.layers.batch_normalization(tf.nn.relu(con1), training=dd)\n",
    "        h_conv1 = batch_norm(tf.nn.relu(con1), scale1, beta1, training=dd)\n",
    "\n",
    "        con2 = tf.nn.conv2d(h_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv2 = tf.nn.relu(tf.layers.batch_normalization(con2, training=b))\n",
    "        #h_conv2 = tf.layers.batch_normalization(tf.nn.relu(con2), training=dd)\n",
    "        h_conv2 = batch_norm(tf.nn.relu(con2), scale2, beta2, training=dd)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        con3 = tf.nn.conv2d(h_pool2, w_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv3 = tf.nn.relu(tf.layers.batch_normalization(con3, training=b))\n",
    "        #h_conv3 = tf.layers.batch_normalization(tf.nn.relu(con3), training=dd)\n",
    "        h_conv3 = batch_norm(tf.nn.relu(con3), scale3, beta3, training=dd)\n",
    "\n",
    "        con4 = tf.nn.conv2d(h_conv3, w_conv4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv4 = tf.nn.relu(tf.layers.batch_normalization(con4, training=b))\n",
    "        #h_conv4 = tf.layers.batch_normalization(tf.nn.relu(con4), training=dd)\n",
    "        h_conv4 = batch_norm(tf.nn.relu(con4), scale4, beta4, training=dd)\n",
    "        h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "        con5 = tf.nn.conv2d(h_pool4, w_conv5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv5 = tf.nn.relu(tf.layers.batch_normalization(con5, training=b))\n",
    "        #h_conv5 = tf.layers.batch_normalization(tf.nn.relu(con5), training=dd)\n",
    "        h_conv5 = batch_norm(tf.nn.relu(con5), scale5, beta5, training=dd)\n",
    "\n",
    "        con6 = tf.nn.conv2d(h_conv5, w_conv6, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv6 = tf.nn.relu(tf.layers.batch_normalization(con6, training=b))\n",
    "        #h_conv6 = tf.layers.batch_normalization(tf.nn.relu(con6), training=dd)\n",
    "        h_conv6 = batch_norm(tf.nn.relu(con6), scale6, beta6, training=dd)\n",
    "\n",
    "        con7 = tf.nn.conv2d(h_conv6, w_conv7, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv7 = tf.nn.relu(tf.layers.batch_normalization(con7, training=b))\n",
    "        #h_conv7 = tf.layers.batch_normalization(tf.nn.relu(con7), training=dd)\n",
    "        h_conv7 = batch_norm(tf.nn.relu(con7), scale7, beta7, training=dd)\n",
    "        h_pool7 = max_pool_2x2(h_conv7)\n",
    "\n",
    "        con8 = tf.nn.conv2d(h_pool7, w_conv8, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv8 = tf.nn.relu(tf.layers.batch_normalization(con8, training=b))\n",
    "        #h_conv8 = tf.layers.batch_normalization(tf.nn.relu(con8), training=dd)\n",
    "        h_conv8 = batch_norm(tf.nn.relu(con8), scale8, beta8, training=dd)\n",
    "\n",
    "        con9 = tf.nn.conv2d(h_conv8, w_conv9, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv9 = tf.nn.relu(tf.layers.batch_normalization(con9, training=b))\n",
    "        #h_conv9 = tf.layers.batch_normalization(tf.nn.relu(con9), training=dd)\n",
    "        h_conv9 = batch_norm(tf.nn.relu(con9), scale9, beta9, training=dd)\n",
    "\n",
    "        con10 = tf.nn.conv2d(h_conv9, w_conv10, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv10 = tf.nn.relu(tf.layers.batch_normalization(con10, training=b))\n",
    "        #h_conv10 = tf.layers.batch_normalization(tf.nn.relu(con10), training=dd)\n",
    "        h_conv10 = batch_norm(tf.nn.relu(con10), scale10, beta10, training=dd)\n",
    "        h_pool10 = max_pool_2x2(h_conv10)\n",
    "\n",
    "        con11 = tf.nn.conv2d(h_pool10, w_conv11, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv11 = tf.nn.relu(tf.layers.batch_normalization(con11, training=b))\n",
    "        #h_conv11 = tf.layers.batch_normalization(tf.nn.relu(con11), training=dd)\n",
    "        h_conv11 = batch_norm(tf.nn.relu(con11), scale11, beta11, training=dd)\n",
    "\n",
    "        con12 = tf.nn.conv2d(h_conv11, w_conv12, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv12 = tf.nn.relu(tf.layers.batch_normalization(con12, training=b))\n",
    "        #h_conv12 = tf.layers.batch_normalization(tf.nn.relu(con12), training=dd)\n",
    "        h_conv12 = batch_norm(tf.nn.relu(con12), scale12, beta12, training=dd)\n",
    "\n",
    "        con13 = tf.nn.conv2d(h_conv12, w_conv13, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv13 = tf.nn.relu(tf.layers.batch_normalization(con13, training=b))\n",
    "        #h_conv13 = tf.layers.batch_normalization(tf.nn.relu(con13), training=dd)\n",
    "        h_conv13 = batch_norm(tf.nn.relu(con13), scale13, beta13, training=dd)\n",
    "        h_pool13 = max_pool_2x2(h_conv13)\n",
    "\n",
    "        h_pool_flat = tf.layers.flatten(h_pool13)\n",
    "        h_pool_flat1 = tf.stack([h_pool_flat]*n_intergal_sample)\n",
    "\n",
    "        h = tf.nn.relu(tf.matmul(h_pool_flat1, w0) + b0)\n",
    "        #h = tf.nn.dropout(h,rate = 1 - 0.7)\n",
    "        h = tf.nn.relu(tf.matmul(h, w1) + b1)\n",
    "        logits = tf.matmul(h, w2) + b2\n",
    "\n",
    "        return logits#num_sample*batch*10\n",
    "    \n",
    "def network1(img,dd):\n",
    "    with tf.variable_scope('V2', reuse=tf.AUTO_REUSE):\n",
    "        # network\n",
    "        con1 = tf.nn.conv2d(img, w_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv1 = tf.nn.relu(tf.layers.batch_normalization(con1, training=b))\n",
    "        #h_conv1 = tf.layers.batch_normalization(tf.nn.relu(con1), training=dd)\n",
    "        h_conv1 = batch_norm(tf.nn.relu(con1), sc1, bt1, training=dd)\n",
    "\n",
    "        con2 = tf.nn.conv2d(h_conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv2 = tf.nn.relu(tf.layers.batch_normalization(con2, training=b))\n",
    "        #h_conv2 = tf.layers.batch_normalization(tf.nn.relu(con2), training=dd)\n",
    "        h_conv2 = batch_norm(tf.nn.relu(con2), sc2, bt2, training=dd)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "        con3 = tf.nn.conv2d(h_pool2, w_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv3 = tf.nn.relu(tf.layers.batch_normalization(con3, training=b))\n",
    "        #h_conv3 = tf.layers.batch_normalization(tf.nn.relu(con3), training=dd)\n",
    "        h_conv3 = batch_norm(tf.nn.relu(con3), sc3, bt3, training=dd)\n",
    "\n",
    "        con4 = tf.nn.conv2d(h_conv3, w_conv4, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv4 = tf.nn.relu(tf.layers.batch_normalization(con4, training=b))\n",
    "        #h_conv4 = tf.layers.batch_normalization(tf.nn.relu(con4), training=dd)\n",
    "        h_conv4 = batch_norm(tf.nn.relu(con4), sc4, bt4, training=dd)\n",
    "        h_pool4 = max_pool_2x2(h_conv4)\n",
    "\n",
    "        con5 = tf.nn.conv2d(h_pool4, w_conv5, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv5 = tf.nn.relu(tf.layers.batch_normalization(con5, training=b))\n",
    "        #h_conv5 = tf.layers.batch_normalization(tf.nn.relu(con5), training=dd)\n",
    "        h_conv5 = batch_norm(tf.nn.relu(con5), sc5, bt5, training=dd)\n",
    "\n",
    "        con6 = tf.nn.conv2d(h_conv5, w_conv6, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv6 = tf.nn.relu(tf.layers.batch_normalization(con6, training=b))\n",
    "        #h_conv6 = tf.layers.batch_normalization(tf.nn.relu(con6), training=dd)\n",
    "        h_conv6 = batch_norm(tf.nn.relu(con6), sc6, bt6, training=dd)\n",
    "\n",
    "        con7 = tf.nn.conv2d(h_conv6, w_conv7, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv7 = tf.nn.relu(tf.layers.batch_normalization(con7, training=b))\n",
    "        #h_conv7 = tf.layers.batch_normalization(tf.nn.relu(con7), training=dd)\n",
    "        h_conv7 = batch_norm(tf.nn.relu(con7), sc7, bt7, training=dd)\n",
    "        h_pool7 = max_pool_2x2(h_conv7)\n",
    "\n",
    "        con8 = tf.nn.conv2d(h_pool7, w_conv8, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv8 = tf.nn.relu(tf.layers.batch_normalization(con8, training=b))\n",
    "        #h_conv8 = tf.layers.batch_normalization(tf.nn.relu(con8), training=dd)\n",
    "        h_conv8 = batch_norm(tf.nn.relu(con8), sc8, bt8, training=dd)\n",
    "\n",
    "        con9 = tf.nn.conv2d(h_conv8, w_conv9, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv9 = tf.nn.relu(tf.layers.batch_normalization(con9, training=b))\n",
    "        #h_conv9 = tf.layers.batch_normalization(tf.nn.relu(con9), training=dd)\n",
    "        h_conv9 = batch_norm(tf.nn.relu(con9), sc9, bt9, training=dd)\n",
    "\n",
    "        con10 = tf.nn.conv2d(h_conv9, w_conv10, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv10 = tf.nn.relu(tf.layers.batch_normalization(con10, training=b))\n",
    "        #h_conv10 = tf.layers.batch_normalization(tf.nn.relu(con10), training=dd)\n",
    "        h_conv10 = batch_norm(tf.nn.relu(con10), sc10, bt10, training=dd)\n",
    "        h_pool10 = max_pool_2x2(h_conv10)\n",
    "\n",
    "        con11 = tf.nn.conv2d(h_pool10, w_conv11, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv11 = tf.nn.relu(tf.layers.batch_normalization(con11, training=b))\n",
    "        #h_conv11 = tf.layers.batch_normalization(tf.nn.relu(con11), training=dd)\n",
    "        h_conv11 = batch_norm(tf.nn.relu(con11), sc11, bt11, training=dd)\n",
    "\n",
    "        con12 = tf.nn.conv2d(h_conv11, w_conv12, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv12 = tf.nn.relu(tf.layers.batch_normalization(con12, training=b))\n",
    "        #h_conv12 = tf.layers.batch_normalization(tf.nn.relu(con12), training=dd)\n",
    "        h_conv12 = batch_norm(tf.nn.relu(con12), sc12, bt12, training=dd)\n",
    "\n",
    "        con13 = tf.nn.conv2d(h_conv12, w_conv13, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        #h_conv13 = tf.nn.relu(tf.layers.batch_normalization(con13, training=b))\n",
    "        #h_conv13 = tf.layers.batch_normalization(tf.nn.relu(con13), training=dd)\n",
    "        h_conv13 = batch_norm(tf.nn.relu(con13), sc13, bt13, training=dd)\n",
    "        h_pool13 = max_pool_2x2(h_conv13)\n",
    "\n",
    "        h_pool_flat = tf.layers.flatten(h_pool13)\n",
    "        h_pool_flat1 = tf.stack([h_pool_flat]*n_intergal_sample)\n",
    "\n",
    "        h = tf.nn.relu(tf.matmul(h_pool_flat1, w0) + b0)\n",
    "        #h = tf.nn.dropout(h,rate = 1 - 0.7)\n",
    "        h = tf.nn.relu(tf.matmul(h, w1) + b1)\n",
    "        logits = tf.matmul(h, w2) + b2\n",
    "\n",
    "        return logits#num_sample*batch*10\n",
    "\n",
    "\n",
    "M = 128\n",
    "n_intergal_sample = 600\n",
    "D=7*7*64\n",
    "h1=512\n",
    "D2=10\n",
    "max_auc = 0\n",
    "\n",
    "#for d in ['/device:GPU:0','/device:GPU:1','/device:GPU:2', '/device:GPU:3']:\n",
    "for d in ['/device:GPU:0']:\n",
    "    with tf.device(d):\n",
    "        def max_pool_2x2(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        # variables\n",
    "        noise = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "        X = tf.placeholder(tf.float32, shape = [None, 32, 32, 3])\n",
    "        Y = tf.placeholder(tf.float32, [None,10])\n",
    "        b = tf.placeholder(tf.float32,shape=(),name='b')\n",
    "        learning_rate = tf.placeholder(tf.float32,shape=(),name='learning_rate')\n",
    "        \n",
    "        scale1 = tf.Variable(tf.ones([64]), trainable=False)\n",
    "        beta1 = tf.Variable(tf.zeros([64]), trainable=False)\n",
    "        scale2 = tf.Variable(tf.ones([64]), trainable=False)\n",
    "        beta2 = tf.Variable(tf.zeros([64]), trainable=False)\n",
    "        scale3 = tf.Variable(tf.ones([128]), trainable=False)\n",
    "        beta3 = tf.Variable(tf.zeros([128]), trainable=False)\n",
    "        scale4 = tf.Variable(tf.ones([128]), trainable=False)\n",
    "        beta4 = tf.Variable(tf.zeros([128]), trainable=False)\n",
    "        scale5 = tf.Variable(tf.ones([256]), trainable=False)\n",
    "        beta5 = tf.Variable(tf.zeros([256]), trainable=False)\n",
    "        scale6 = tf.Variable(tf.ones([256]), trainable=False)\n",
    "        beta6 = tf.Variable(tf.zeros([256]), trainable=False)\n",
    "        scale7 = tf.Variable(tf.ones([256]), trainable=False)\n",
    "        beta7 = tf.Variable(tf.zeros([256]), trainable=False)\n",
    "        scale8 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        beta8 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        scale9 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        beta9 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        scale10 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        beta10 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        scale11 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        beta11 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        scale12 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        beta12 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        scale13 = tf.Variable(tf.ones([512]), trainable=False)\n",
    "        beta13 = tf.Variable(tf.zeros([512]), trainable=False)\n",
    "        \n",
    "        bt1 = tf.stop_gradient(tf.identity(beta1))\n",
    "        sc1 = tf.stop_gradient(tf.identity(scale1))\n",
    "        bt2 = tf.stop_gradient(tf.identity(beta2))\n",
    "        sc2 = tf.stop_gradient(tf.identity(scale2))\n",
    "        bt3 = tf.stop_gradient(tf.identity(beta3))\n",
    "        sc3 = tf.stop_gradient(tf.identity(scale3))\n",
    "        bt4 = tf.stop_gradient(tf.identity(beta4))\n",
    "        sc4 = tf.stop_gradient(tf.identity(scale4))\n",
    "        bt5 = tf.stop_gradient(tf.identity(beta5))\n",
    "        sc5 = tf.stop_gradient(tf.identity(scale5))\n",
    "        bt6 = tf.stop_gradient(tf.identity(beta6))\n",
    "        sc6 = tf.stop_gradient(tf.identity(scale6))        \n",
    "        bt7 = tf.stop_gradient(tf.identity(beta7))\n",
    "        sc7 = tf.stop_gradient(tf.identity(scale7))        \n",
    "        bt8 = tf.stop_gradient(tf.identity(beta8))\n",
    "        sc8 = tf.stop_gradient(tf.identity(scale8))        \n",
    "        bt9 = tf.stop_gradient(tf.identity(beta9))\n",
    "        sc9 = tf.stop_gradient(tf.identity(scale9))\n",
    "        bt10 = tf.stop_gradient(tf.identity(beta10))\n",
    "        sc10 = tf.stop_gradient(tf.identity(scale10))        \n",
    "        bt11 = tf.stop_gradient(tf.identity(beta11))\n",
    "        sc11 = tf.stop_gradient(tf.identity(scale11))\n",
    "        bt12 = tf.stop_gradient(tf.identity(beta12))\n",
    "        sc12 = tf.stop_gradient(tf.identity(scale12))        \n",
    "        bt13 = tf.stop_gradient(tf.identity(beta13))\n",
    "        sc13 = tf.stop_gradient(tf.identity(scale13))\n",
    "\n",
    "        w_conv1 = tf.get_variable('w_conv1', [3,3,3,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv2 = tf.get_variable('w_conv2', [3,3,64,64], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv3 = tf.get_variable('w_conv3', [3,3,64,128], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv4 = tf.get_variable('w_conv4', [3,3,128,128], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv5 = tf.get_variable('w_conv5', [3,3,128,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv6 = tf.get_variable('w_conv6', [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv7 = tf.get_variable('w_conv7', [3,3,256,256], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv8 = tf.get_variable('w_conv8', [3,3,256,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv9 = tf.get_variable('w_conv9', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv10 = tf.get_variable('w_conv10', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w_conv11 = tf.get_variable('w_conv11', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv12 = tf.get_variable('w_conv12', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w_conv13 = tf.get_variable('w_conv13', [3,3,512,512], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w0_m = tf.get_variable('w_fc1', [1*1*512, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b0_m = tf.get_variable('b_fc1', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w1_m = tf.get_variable('w_fc2', [1024, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1_m = tf.get_variable('b_fc2', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w2_m = tf.get_variable('w_fc3', [1024, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2_m = tf.get_variable('b_fc3', [1,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        w0_r = tf.get_variable('w_fc1_r', [1*1*512, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b0_r = tf.get_variable('b_fc1_r', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w1_r = tf.get_variable('w_fc2_r', [1024, 1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b1_r = tf.get_variable('b_fc2_r', [1,1024], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        w2_r = tf.get_variable('w_fc3_r', [1024, 10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "        b2_r = tf.get_variable('b_fc3_r', [1,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        w0_sigma = w0_r**2\n",
    "        b0_sigma = b0_r**2\n",
    "        w1_sigma = w1_r**2\n",
    "        b1_sigma = b1_r**2\n",
    "        w2_sigma = w2_r**2\n",
    "        b2_sigma = b2_r**2\n",
    "\n",
    "        eps1 = tf.random_normal(shape=[n_intergal_sample,1*1*512, 1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps2 = tf.random_normal(shape=[n_intergal_sample,1,1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps3 = tf.random_normal(shape=[n_intergal_sample,1024, 1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps4 = tf.random_normal(shape=[n_intergal_sample,1,1024], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps5 = tf.random_normal(shape=[n_intergal_sample,1024, 10], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "        eps6 = tf.random_normal(shape=[n_intergal_sample,1,10], mean=0.0, stddev=1,dtype=tf.float32)\n",
    "\n",
    "        w0 = w0_m + eps1 * w0_sigma\n",
    "        b0 = b0_m + eps2 * b0_sigma\n",
    "        w1 = w1_m + eps3 * w1_sigma\n",
    "        b1 = b1_m + eps4 * b1_sigma\n",
    "        w2 = w2_m + eps5 * w2_sigma\n",
    "        b2 = b2_m + eps6 * b2_sigma\n",
    "\n",
    "        # network\n",
    "\n",
    "\n",
    "        #evaluation\n",
    "        ###训练接口\n",
    "        logits0 = network(X,True)\n",
    "        logits = tf.reduce_mean(logits0,0)\n",
    "        output0 = tf.nn.softmax(logits0)\n",
    "        cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=Y)\n",
    "        \n",
    "        probs = tf.reduce_sum(output0*Y,2)\n",
    "        prob=tf.reduce_mean(probs,0)#积分\n",
    "        log_prob = tf.log(prob+1e-30)#取log\n",
    "        #p = tf.reduce_mean(log_prob)#求和\n",
    "        p = tf.reduce_mean(-cross_ent)\n",
    "\n",
    "        output, var0 = tf.nn.moments(output0,0)#batch*10\n",
    "        prob1 = tf.reduce_sum(output*Y,1)\n",
    "        max_p = tf.reduce_max(output,1)\n",
    "        ent = tf.reduce_sum(-tf.log(output+1e-30)*output,1)\n",
    "        Eent = tf.reduce_mean(tf.reduce_sum(-tf.log(output0+1e-30)*output0,2),0)\n",
    "        MI = ent - Eent\n",
    "        MI_mean = tf.reduce_sum(MI)\n",
    "\n",
    "        correct_pred = tf.equal(tf.argmax(output,1), tf.argmax(Y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        \n",
    "        ###测试接口\n",
    "        logits2 = network(X,False)\n",
    "        logits22 = tf.reduce_mean(logits2,0)\n",
    "        cross_ent_test = tf.nn.softmax_cross_entropy_with_logits(logits = logits22, labels=Y)\n",
    "        p_test = tf.reduce_mean(-cross_ent_test)\n",
    "        \n",
    "        output2 = tf.nn.softmax(logits2)\n",
    "        output_test, var_test = tf.nn.moments(output2,0)#batch*10\n",
    "        max_p_test = tf.reduce_max(output_test,1)\n",
    "        ent_test = tf.reduce_sum(-tf.log(output_test+1e-30)*output_test,1)\n",
    "        Eent_test = tf.reduce_mean(tf.reduce_sum(-tf.log(output2+1e-30)*output2,2),0)\n",
    "        MI_test = ent_test - Eent_test\n",
    "        MI_mean_test = tf.reduce_sum(MI_test)\n",
    "\n",
    "        correct_pred_test = tf.equal(tf.argmax(output_test,1), tf.argmax(Y,1))\n",
    "        accuracy_test = tf.reduce_mean(tf.cast(correct_pred_test, tf.float32))\n",
    "\n",
    "        ##noise接口\n",
    "        logits1 = network1(noise,True)\n",
    "        output1 = tf.nn.softmax(logits1)\n",
    "        output_noise, varent_noise = tf.nn.moments(output1,0)\n",
    "        max_p_noise = tf.reduce_max(output_noise,1)\n",
    "        ent_noise = tf.reduce_sum(-tf.log(output_noise+1e-30)*output_noise,1)\n",
    "        Eent_noise = tf.reduce_mean(tf.reduce_sum(-tf.log(output1+1e-30)*output1,2),0)\n",
    "        MI_noise = ent_noise - Eent_noise\n",
    "        MI_noise_mean = tf.reduce_sum(MI_noise)\n",
    "        \n",
    "        correct_pred_noise = tf.equal(tf.argmax(output_noise,1), tf.argmax(Y,1))\n",
    "        accuracy_noise = tf.reduce_mean(tf.cast(correct_pred_noise, tf.float32))\n",
    "\n",
    "        regularization = 1e-6*(tf.reduce_sum(tf.square(w0_m))+tf.reduce_sum(tf.square(b0_m))\n",
    "                               +tf.reduce_sum(tf.square(w1_m))+tf.reduce_sum(tf.square(b1_m))\n",
    "                               +tf.reduce_sum(tf.square(w2_m))+tf.reduce_sum(tf.square(b2_m))\n",
    "                               +tf.reduce_sum(tf.square(w_conv1))+tf.reduce_sum(tf.square(w_conv2))\n",
    "                               +tf.reduce_sum(tf.square(w_conv3))+tf.reduce_sum(tf.square(w_conv4))\n",
    "                               +tf.reduce_sum(tf.square(w_conv5))+tf.reduce_sum(tf.square(w_conv6))+tf.reduce_sum(tf.square(w_conv7))\n",
    "                               +tf.reduce_sum(tf.square(w_conv8))+tf.reduce_sum(tf.square(w_conv9))+tf.reduce_sum(tf.square(w_conv10))\n",
    "                               +tf.reduce_sum(tf.square(w_conv11))+tf.reduce_sum(tf.square(w_conv12))+tf.reduce_sum(tf.square(w_conv13)))\n",
    "\n",
    "        loss = -p*1 + regularization*1 - MI_noise_mean*0.02*0\n",
    "        loss_test = -p_test + regularization - MI_noise_mean*0.02\n",
    "# trainin\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "with tf.Session(config = config) as sess:\n",
    "    start_time = time.time()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model_id=0\n",
    "    #save_path = saver.restore(sess, \"./density_estimation_cifar10/classification_mnist4/model.ckpt\" )\n",
    "    for i in range(180000):\n",
    "        \n",
    "        start_time1 = time.time()\n",
    "        seed = np.random.randint(0,50000,M)\n",
    "        x_batch = x_train[seed]\n",
    "        y_batch = y_train[seed]\n",
    "        x_batch = preprocess(x_batch)\n",
    "        #my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "        my_noise = np.reshape(np.random.normal(0,1,[32*32*3*M]),[M,32,32,3])\n",
    "        #my_noise = TIM[np.random.randint(0,10000,M)]\n",
    "        #my_noise = stand(my_noise)\n",
    "        \n",
    "        #pp = sess.run(output_test,{X: x_batch, Y: y_batch,noise:my_noise, b:False})\n",
    "            \n",
    "        if i%1000==0 and i!=0:\n",
    "            loss_now = sess.run(loss_test,{X: x_batch, Y: y_batch,noise:my_noise, b:1.})\n",
    "            loss_now1 = sess.run(MI_noise_mean*0.2,{X: x_batch, Y: y_batch,noise:my_noise, b:1.})\n",
    "            print(\"loss\",loss_now,\"-p+regular \",loss_now+loss_now1,\"MI\",loss_now1)\n",
    "            \n",
    "            train_accuracy=[]\n",
    "            seed1 = np.random.randint(0,50000,10000)\n",
    "            x_vali = x_train[seed1]\n",
    "            y_vali = y_train[seed1]\n",
    "            for j in range(0,100):\n",
    "                imgs_in = x_vali[j*100:(j+1)*100]\n",
    "                labels_in = y_vali[j*100:(j+1)*100]\n",
    "                imgs_in=stand(imgs_in)\n",
    "                acc0 = sess.run(accuracy_test,{X:imgs_in,Y:labels_in,noise:imgs_in,b:1.})\n",
    "                train_accuracy.append(acc0)\n",
    "            train_accuracy = np.array(train_accuracy)\n",
    "            train_accuracy = np.mean(train_accuracy)\n",
    "            \n",
    "            test_accuracy=[]\n",
    "            maxp_in = []\n",
    "            ent_in = []\n",
    "            MI_in = []\n",
    "            for j in range(0,100):\n",
    "                imgs_in = x_test[j*100:(j+1)*100]\n",
    "                labels_in = y_test[j*100:(j+1)*100]\n",
    "                imgs_in=stand(imgs_in)\n",
    "                #imgs_in=stand1(imgs_in)\n",
    "                acc = sess.run(accuracy_test,{X:imgs_in,Y:labels_in,noise:imgs_in,b:1.})\n",
    "                maxp_in1 = sess.run(max_p_test,{X:imgs_in,noise:imgs_in,b:1.})\n",
    "                ent_in1 = sess.run(ent_test,{X:imgs_in,noise:imgs_in,b:1.})\n",
    "                MI_in1 = sess.run(MI_test,{X:imgs_in,noise:imgs_in,b:1.})\n",
    "                test_accuracy.append(acc)\n",
    "                maxp_in.extend(maxp_in1)\n",
    "                ent_in.extend(ent_in1)\n",
    "                MI_in.extend(MI_in1)\n",
    "            test_accuracy = np.array(test_accuracy)\n",
    "            test_accuracy = np.mean(test_accuracy)\n",
    "            maxp_in = np.array(maxp_in)\n",
    "            ent_in = np.array(ent_in)\n",
    "            MI_in = np.array(MI_in)\n",
    "            print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy,\"train accuracy\",train_accuracy)\n",
    "\n",
    "            for t in range(0,2):\n",
    "                '''\n",
    "                if t == 0:\n",
    "                    safe_images = np.reshape(np.random.normal( 0.0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                    print(\"gauss noise:\")\n",
    "                \n",
    "                if t == 1:\n",
    "                    safe_images = np.reshape(np.random.uniform(0, 1,[1,9200*784] ),[9200,28,28,1])\n",
    "                    print(\"average noise:\")\n",
    "                '''\n",
    "                if t == 0:\n",
    "                    safe_images = np.copy(TIM)\n",
    "                    print(\"TIM:\")\n",
    "                if t == 1:\n",
    "                    safe_images = np.reshape(np.random.normal(0,1,[32*32*3*10000]),[10000,32,32,3])\n",
    "                    #safe_images = (safe_images - np.min(safe_images))/(np.max(safe_images) - np.min(safe_images))\n",
    "                    print(\"noise:\")\n",
    "                maxp_OOD = []\n",
    "                ent_OOD = []\n",
    "                MI_OOD = []\n",
    "                    \n",
    "                for k in range(0,100):\n",
    "                    imgs_OOD = safe_images[k*100:(k+1)*100]\n",
    "                    imgs_OOD = stand(imgs_OOD)\n",
    "                    #imgs_OOD = stand1(imgs_OOD)\n",
    "                    maxp_OOD1 = sess.run(max_p_test,{X:imgs_OOD,noise:imgs_OOD,b:1.})\n",
    "                    ent_OOD1 = sess.run(ent_test,{X:imgs_OOD,noise:imgs_OOD,b:1.})\n",
    "                    MI_OOD1 = sess.run(MI_test,{X:imgs_OOD,noise:imgs_OOD,b:1.})\n",
    "                    maxp_OOD.extend(maxp_OOD1)\n",
    "                    ent_OOD.extend(ent_OOD1)\n",
    "                    MI_OOD.extend(MI_OOD1)\n",
    "                maxp_OOD = np.array(maxp_OOD)\n",
    "                ent_OOD = np.array(ent_OOD)\n",
    "                MI_OOD = np.array(MI_OOD)\n",
    "                '''\n",
    "                print (\"time:\",i, time.time() - start_time, \"test accuracy\", test_accuracy, \"validation_error\",validation_accuracy)\n",
    "                print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "                '''\n",
    "                print(\"maxp_OOD:\",np.mean(maxp_OOD>0.99),np.mean(maxp_OOD),np.std(maxp_OOD))\n",
    "                print(\"maxp_inD:\",np.mean(maxp_in>0.99),np.mean(maxp_in),np.std(maxp_in))\n",
    "                print(\"ent_OOD:\",np.mean(ent_OOD),np.std(ent_OOD), \"ent_in:\", np.mean(ent_in),np.std(ent_in))\n",
    "                print(\"MI_OOD:\",np.mean(MI_OOD),np.std(MI_OOD), \"MI_in:\", np.mean(MI_in),np.std(MI_in))\n",
    "                \n",
    "                \n",
    "                safe, risky = -np.reshape(maxp_in,[10000,1]), -np.reshape(maxp_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_p:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_p:', round(100*roc_auc_score(labels, examples), 2))\n",
    "\n",
    "                safe, risky = np.reshape(ent_in,[10000,1]), np.reshape(ent_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_entropy:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_entropy:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                if t == 0:\n",
    "                    tmp_indicator = round(100*roc_auc_score(labels, examples), 2)\n",
    "\n",
    "                safe, risky = np.reshape(MI_in,[10000,1]), np.reshape(MI_OOD,[10000,1])\n",
    "                labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "                labels[safe.shape[0]:] += 1\n",
    "                examples = np.squeeze(np.vstack((safe, risky)))\n",
    "                print('AUPR_MI:', round(100*average_precision_score(labels, examples), 2))\n",
    "                print('AUROC_MI:', round(100*roc_auc_score(labels, examples), 2))\n",
    "                print(\"----------------------------------------\")\n",
    "            \n",
    "                \n",
    "        if i < 10000:\n",
    "            a = 0.001\n",
    "        elif i < 20000:\n",
    "            a = 0.0003\n",
    "        elif i < 30000:\n",
    "            a = 0.0001\n",
    "        elif i < 40000:\n",
    "            a = 0.00003\n",
    "        elif i < 50000:\n",
    "            a = 0.00001\n",
    "        elif i < 60000:\n",
    "            a = 0.000003\n",
    "        elif i < 70000:\n",
    "            a = 0.000001\n",
    "        elif i < 80000:\n",
    "            a = 0.0000003\n",
    "        else:\n",
    "            a = 0.0000001\n",
    "\n",
    "        sess.run(train_step,{X: x_batch, Y: y_batch,noise:my_noise, b:2., learning_rate:a*.2})\n",
    "        if i%1000==0:\n",
    "            save_path = saver.save(sess, \"./density_estimation_cifar10/classification_mnist%s/model.ckpt\" % model_id)\n",
    "            print(\"model\", model_id,\"saved0.02\")\n",
    "            model_id+=1 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
